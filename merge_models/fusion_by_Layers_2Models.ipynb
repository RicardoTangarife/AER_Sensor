{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1XB6mQecSFV7IgJsb_nwrUkgYN-cK__Q9","authorship_tag":"ABX9TyM1JWMIop5Kk1pgmU9nnOSx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Reload a fresh Keras model from the saved model"],"metadata":{"id":"LmG6iaVHAx9Z"}},{"cell_type":"code","source":["import IPython.display\n","import librosa\n","import librosa.display\n","import pandas as pd\n","import os\n","import struct\n","import glob\n","import soundfile as sf\n","from tqdm import tqdm\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib.pyplot import specgram\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","import tensorflow as tf\n","from keras.callbacks import ModelCheckpoint\n","from datetime import datetime\n","from sklearn import metrics\n","import librosa as lb\n","import IPython.display as ipd\n","from tensorflow import keras\n","from sklearn.metrics import confusion_matrix, classification_report\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, Flatten, Dense, Dropout, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization\n","from tqdm import tqdm\n","\n","seed = 42\n","tf.random.set_seed(seed)\n","np.random.seed(seed)"],"metadata":{"id":"sERj-GtVA0U8","executionInfo":{"status":"ok","timestamp":1696396567548,"user_tz":300,"elapsed":345,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}}},"execution_count":168,"outputs":[]},{"cell_type":"markdown","source":["#PREPARE SCREAM DATASET"],"metadata":{"id":"pgDvIhTzJEZB"}},{"cell_type":"code","source":["#DATASET USED\n","#https://www.kaggle.com/datasets/sanzidaakterarusha/scream-dataset?datasetId=1903423&sortBy=dateRun&tab=collaboration\n","\n","# Se obtienen las diferentes rutas de los datos, tanto audios como metadata y path para obtener las muestras\n","file_path = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/UrbanSounds8k/UrbanSound8K/audio'\n","screams_path = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/sounds-screams/good'\n","urbansound8k = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/UrbanSounds8k/UrbanSound8K/metadata/UrbanSound8K.csv')\n","file_viz = glob.glob('/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/UrbanSounds8k/UrbanSound8K/audio/fold1/*')"],"metadata":{"id":"YD2SuWevJCWL","executionInfo":{"status":"ok","timestamp":1696396567923,"user_tz":300,"elapsed":9,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}}},"execution_count":169,"outputs":[]},{"cell_type":"code","source":["#pd.set_option('display.max_rows', None)\n","urbansound8k.head()"],"metadata":{"id":"XhRLkfFUNFNY","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1696396567923,"user_tz":300,"elapsed":8,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}},"outputId":"fa579346-cc12-459e-c812-cb164646bc37"},"execution_count":170,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      slice_file_name    fsID  start        end  salience  fold  classID  \\\n","0    100032-3-0-0.wav  100032    0.0   0.317551         1     5        3   \n","1  100263-2-0-117.wav  100263   58.5  62.500000         1     5        2   \n","2  100263-2-0-121.wav  100263   60.5  64.500000         1     5        2   \n","3  100263-2-0-126.wav  100263   63.0  67.000000         1     5        2   \n","4  100263-2-0-137.wav  100263   68.5  72.500000         1     5        2   \n","\n","              class  \n","0          dog_bark  \n","1  children_playing  \n","2  children_playing  \n","3  children_playing  \n","4  children_playing  "],"text/html":["\n","  <div id=\"df-7f31561a-6527-4fab-bdd6-61410121acf1\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>slice_file_name</th>\n","      <th>fsID</th>\n","      <th>start</th>\n","      <th>end</th>\n","      <th>salience</th>\n","      <th>fold</th>\n","      <th>classID</th>\n","      <th>class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>100032-3-0-0.wav</td>\n","      <td>100032</td>\n","      <td>0.0</td>\n","      <td>0.317551</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>dog_bark</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>100263-2-0-117.wav</td>\n","      <td>100263</td>\n","      <td>58.5</td>\n","      <td>62.500000</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>children_playing</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>100263-2-0-121.wav</td>\n","      <td>100263</td>\n","      <td>60.5</td>\n","      <td>64.500000</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>children_playing</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>100263-2-0-126.wav</td>\n","      <td>100263</td>\n","      <td>63.0</td>\n","      <td>67.000000</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>children_playing</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>100263-2-0-137.wav</td>\n","      <td>100263</td>\n","      <td>68.5</td>\n","      <td>72.500000</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>children_playing</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7f31561a-6527-4fab-bdd6-61410121acf1')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-7f31561a-6527-4fab-bdd6-61410121acf1 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-7f31561a-6527-4fab-bdd6-61410121acf1');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-4ee10040-f173-4f3e-b2ef-8f35dd474190\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4ee10040-f173-4f3e-b2ef-8f35dd474190')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-4ee10040-f173-4f3e-b2ef-8f35dd474190 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":170}]},{"cell_type":"code","source":["#Se organiza dataset para seleccionar la clase gun shot de interes y las otras renombrarse como no scream\n","urbansound8k.loc[urbansound8k[\"class\"] != \" \", \"class\"] = \"non_scream\"\n","#Se cambian los valores de la columna classID para identificar unicamente las dos clases de interes\n","urbansound8k.loc[urbansound8k[\"classID\"] != 10, \"classID\"] = 0 #non_event\n","#se cuenta el numero total de muestras de la clase de interes\n","totalSamples = 0\n","lastNameFilesScreams=[]\n","for file in os.listdir(screams_path):\n","    if file.endswith('.wav'):\n","        lastNameFilesScreams.append(file)\n","        totalSamples+=1\n","print(totalSamples)\n","#se toman las muestras aleatorias del tamaño de las muestras de la clase de interes para que quede balanceado\n","urbansound8k[urbansound8k['class'] == \"non_scream\"].sample(n=totalSamples).head()\n","#se toman una las muestras aleatorias del tamaño de las muestras de la clase de interes para que quede balanceado\n","dfNonScream = urbansound8k[urbansound8k['class'] == \"non_scream\"].sample(n=totalSamples)\n","#Se crea dataframe con los datos de los audios de sonidos de interes\n","dfScream = pd.DataFrame(lastNameFilesScreams, columns=['slice_file_name'])\n","dfScream['classID'] = 1 #scream\n","dfScream['class'] = 'scream'\n","dfScream['fold'] = 0\n","dfScream\n","#se unen en un solo dataset las clases de interes con muestras balanceadas\n","dfCompleteScream = dfNonScream.append(dfScream)\n","dfCompleteScream.count()[0]\n","dfCompleteScream[dfCompleteScream['class'] == \"scream\"].count()[0]\n","#Se cambia el dataset para unicamente tomar las clases de interés para el trabajo\n","dfCompleteScream.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":258},"id":"GEgIsRltNHHj","executionInfo":{"status":"ok","timestamp":1696396568307,"user_tz":300,"elapsed":390,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}},"outputId":"42674876-dd14-4745-8343-464d3256a307"},"execution_count":171,"outputs":[{"output_type":"stream","name":"stdout","text":["391\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-171-a9de57edfae9>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  dfCompleteScream = dfNonScream.append(dfScream)\n"]},{"output_type":"execute_result","data":{"text/plain":["        slice_file_name      fsID      start        end  salience  fold  \\\n","2852  161129-4-0-11.wav  161129.0  13.775687  17.775687       1.0     8   \n","6135    34056-2-0-2.wav   34056.0   1.000000   5.000000       2.0     4   \n","1023   123685-5-0-8.wav  123685.0   7.845506  11.845506       1.0     7   \n","2481  157867-8-0-10.wav  157867.0   5.000000   9.000000       1.0     1   \n","1967    14772-7-9-0.wav   14772.0  31.891222  35.786547       1.0     7   \n","\n","      classID       class  \n","2852        0  non_scream  \n","6135        0  non_scream  \n","1023        0  non_scream  \n","2481        0  non_scream  \n","1967        0  non_scream  "],"text/html":["\n","  <div id=\"df-a360f970-4735-455b-b73b-e2a7451c9018\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>slice_file_name</th>\n","      <th>fsID</th>\n","      <th>start</th>\n","      <th>end</th>\n","      <th>salience</th>\n","      <th>fold</th>\n","      <th>classID</th>\n","      <th>class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2852</th>\n","      <td>161129-4-0-11.wav</td>\n","      <td>161129.0</td>\n","      <td>13.775687</td>\n","      <td>17.775687</td>\n","      <td>1.0</td>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>non_scream</td>\n","    </tr>\n","    <tr>\n","      <th>6135</th>\n","      <td>34056-2-0-2.wav</td>\n","      <td>34056.0</td>\n","      <td>1.000000</td>\n","      <td>5.000000</td>\n","      <td>2.0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>non_scream</td>\n","    </tr>\n","    <tr>\n","      <th>1023</th>\n","      <td>123685-5-0-8.wav</td>\n","      <td>123685.0</td>\n","      <td>7.845506</td>\n","      <td>11.845506</td>\n","      <td>1.0</td>\n","      <td>7</td>\n","      <td>0</td>\n","      <td>non_scream</td>\n","    </tr>\n","    <tr>\n","      <th>2481</th>\n","      <td>157867-8-0-10.wav</td>\n","      <td>157867.0</td>\n","      <td>5.000000</td>\n","      <td>9.000000</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>non_scream</td>\n","    </tr>\n","    <tr>\n","      <th>1967</th>\n","      <td>14772-7-9-0.wav</td>\n","      <td>14772.0</td>\n","      <td>31.891222</td>\n","      <td>35.786547</td>\n","      <td>1.0</td>\n","      <td>7</td>\n","      <td>0</td>\n","      <td>non_scream</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a360f970-4735-455b-b73b-e2a7451c9018')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-a360f970-4735-455b-b73b-e2a7451c9018 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-a360f970-4735-455b-b73b-e2a7451c9018');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-a86335ae-e543-46db-86eb-92ea9e4f5c77\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a86335ae-e543-46db-86eb-92ea9e4f5c77')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-a86335ae-e543-46db-86eb-92ea9e4f5c77 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":171}]},{"cell_type":"code","source":["#Se define funcion para extrar las caracteristicas con la librería librosa, obetiendo los coeficientes ceptrales de frecuencia de Mel\n","#Se realiza un pading en el tamaño para que concuerden los tamaños de las caracteristicas de entrada al modelo.(?)\n","def extract_features(file_name, Nmfcc, Nfft, NhopL, NwinL):\n","\n","    samplerate = 22050\n","    longitudMaxAudio = 4\n","    max_pad_len = int(samplerate*longitudMaxAudio/NhopL) + int(samplerate*longitudMaxAudio/NhopL*0.05)  #Calculo longitud de salida de mfcc con 5% de tolerancia para longitud de audios\n","\n","    try:\n","      audio, sample_rate = librosa.load(file_name, res_type='soxr_hq')\n","      mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=Nmfcc, n_fft=Nfft, hop_length=NhopL, win_length=NwinL)\n","      pad_width = max_pad_len - mfccs.shape[1]\n","      mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n","\n","    except Exception as e:\n","      print(\"Error encountered while parsing file: \", file_name)\n","      return None\n","    #print(mfccs.shape)\n","    return mfccs"],"metadata":{"id":"qNRIdkFQX0zT","executionInfo":{"status":"ok","timestamp":1696396568307,"user_tz":300,"elapsed":8,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}}},"execution_count":172,"outputs":[]},{"cell_type":"code","source":["#Se realiza la extracción de caracteristicas, teniendo en cuenta la clase, si el sonido es de la carpeta agregada de la clase explosions va y busca este sonido en la carpeta requerida\n","\n","def get_features(Nmfcc, Nfft, NhopL, NwinL):\n","  features = []\n","  # Iterate through each sound file and extract the features\n","  for index, row in dfCompleteScream.iterrows():\n","      if(row[\"class\"]==\"non_scream\"):\n","          file_name = os.path.join(os.path.abspath(file_path),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n","          #file_name = \"\"\n","      elif(row[\"class\"]==\"scream\"):\n","          file_name = os.path.join(os.path.abspath(screams_path),str(row[\"slice_file_name\"]))\n","          #file_name = \"\"\n","      #print(file_name)\n","      class_label = row[\"classID\"]\n","      data = extract_features(file_name, Nmfcc, Nfft, NhopL, NwinL)\n","\n","      features.append([data, class_label])\n","\n","  # Convert into a Panda dataframe\n","  featuresdf = pd.DataFrame(features, columns=['feature','class_label'])\n","  return featuresdf"],"metadata":{"id":"0CdgBpxRX4Rl","executionInfo":{"status":"ok","timestamp":1696396568308,"user_tz":300,"elapsed":9,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}}},"execution_count":173,"outputs":[]},{"cell_type":"code","source":["def splitFeaturesTrainTest(featuresdf, num_rows, num_columns, num_channels):\n","  X = np.array(featuresdf.feature.tolist())\n","  y = np.array(featuresdf.class_label.tolist())\n","\n","  # Encode the classification labels\n","  le = LabelEncoder()\n","  yy = to_categorical(le.fit_transform(y))\n","\n","  # split the dataset\n","  from sklearn.model_selection import train_test_split\n","\n","  x_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 3)\n","\n","  x_train = x_train.reshape(x_train.shape[0], num_rows, num_columns, num_channels)\n","  x_test = x_test.reshape(x_test.shape[0], num_rows, num_columns, num_channels)\n","  num_labels = yy.shape[1]\n","  return x_train, x_test, y_train, y_test, num_labels"],"metadata":{"id":"WRu5eoaXX6uP","executionInfo":{"status":"ok","timestamp":1696396568308,"user_tz":300,"elapsed":8,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}}},"execution_count":174,"outputs":[]},{"cell_type":"code","source":["\n","\n","#1\t3\t4096\t4096\t4096\t0\t2\n","#2\t12\t4096\t4096\t4096\t0\t3\n","#3\t27\t4096\t4096\t4096\t0\t3\n","#4\t45\t4096\t4096\t4096\t0\t5\n","\n","\n","samplerate = 22050\n","longitudMaxAudio = 4\n","Nmfcc = 3\n","Nfft = 4096\n","NwinL = 4096\n","iterableNhopL = 1.0\n","NhopL =  4096       #int(iterableNhopL*NwinL)\n","k_size = 2\n","num_rows = Nmfcc\n","num_columns = int(samplerate*longitudMaxAudio/NhopL) + int(samplerate*longitudMaxAudio/NhopL*0.05)  #Calculo longitud de salida de mfcc con 5% de tolerancia para longitud de audios\n","num_channels = 1\n","\n","\n","featuresdfScream = get_features(Nmfcc, Nfft, NhopL, NwinL)\n","x_trainScream, x_testScream, y_trainScream, y_testScream, num_labelsScream = splitFeaturesTrainTest(featuresdfScream, num_rows, num_columns, num_channels)\n"],"metadata":{"id":"qTj37J7aX7q-","executionInfo":{"status":"ok","timestamp":1696396596666,"user_tz":300,"elapsed":28366,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}}},"execution_count":175,"outputs":[]},{"cell_type":"markdown","source":["#PREPARE GUNSHOT DATASET"],"metadata":{"id":"fbGSTnkIGWIQ"}},{"cell_type":"code","source":["# Se obtienen las diferentes rutas de los datos, tanto audios como metadata y path para obtener las muestras\n","file_path = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/UrbanSounds8k/UrbanSound8K/audio'\n","explosions_path = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/sounds-explosions/explosions'\n","urbansound8k = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/UrbanSounds8k/UrbanSound8K/metadata/UrbanSound8K.csv')\n","file_viz = glob.glob('/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/UrbanSounds8k/UrbanSound8K/audio/fold1/*')"],"metadata":{"id":"R4oujrcSGXuI","executionInfo":{"status":"ok","timestamp":1696396596666,"user_tz":300,"elapsed":30,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}}},"execution_count":176,"outputs":[]},{"cell_type":"code","source":["#Se organiza dataset para seleccionar la clase gun shot de interes y las otras renombrarse como no gunshot\n","urbansound8k.loc[urbansound8k[\"class\"] != \"gun_shot\", \"class\"] = \"non_gun_shot\"\n","#Se cambian los valores de la columna classID para identificar unicamente las dos clases de interes\n","urbansound8k.loc[urbansound8k[\"classID\"] != 6, \"classID\"] = 0  #non_event\n","urbansound8k.loc[urbansound8k[\"classID\"] == 6, \"classID\"] = 1  #gun_shot\n","\n","#se cuenta el numero total de muestras de la clase de interes\n","totalSamplesGunshot =  urbansound8k.loc[((urbansound8k[\"class\"]==\"gun_shot\"))].count()[0]\n","totalSamplesGunshot\n","\n","#se carga el numero total de muestras de la clase a agregar al dataset - explosions\n","totalSamplesExplosions = 0\n","lastNameFilesExplosions=[]\n","for file in os.listdir(explosions_path):\n","    if file.endswith('.wav'):\n","        lastNameFilesExplosions.append(file)\n","        totalSamplesExplosions+=1\n","print(totalSamplesExplosions)\n","\n","#Se crea dataframe con los datos de los audios de sonidos de la clase a agregar - explosions\n","dfExplosion = pd.DataFrame(lastNameFilesExplosions, columns=['slice_file_name'])\n","dfExplosion['classID'] = 0 #non_event\n","dfExplosion['class'] = 'non_gun_shot'\n","dfExplosion['fold'] = 0\n","\n","#Agregar en un dataset todos los sonidos de interes con la clase nueva agregada - explosions\n","print(urbansound8k.count()[0])\n","urbansound8k = urbansound8k.append(dfExplosion)\n","urbansound8k.count()[0]\n","\n","#se toma una muestra aleatoria del tamaño de las muestras de la clase de interes para que quede balanceado\n","dfNonGunShot = urbansound8k[urbansound8k['class'] == \"non_gun_shot\"].sample(n=totalSamplesGunshot)\n","dfGunShot= urbansound8k[urbansound8k['class'] == \"gun_shot\"]\n","\n","#se unen en un solo dataset las clases de interes con muestras balanceadas\n","dfCompleteGunshot = dfNonGunShot.append(dfGunShot)\n","dfCompleteGunshot.count()[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HocOYEtnGZ_z","executionInfo":{"status":"ok","timestamp":1696396596667,"user_tz":300,"elapsed":28,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}},"outputId":"d1d065f8-0973-477c-8c85-cc0cb359476d"},"execution_count":177,"outputs":[{"output_type":"stream","name":"stdout","text":["209\n","8732\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-177-5260589046a1>:28: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  urbansound8k = urbansound8k.append(dfExplosion)\n","<ipython-input-177-5260589046a1>:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  dfCompleteGunshot = dfNonGunShot.append(dfGunShot)\n"]},{"output_type":"execute_result","data":{"text/plain":["748"]},"metadata":{},"execution_count":177}]},{"cell_type":"code","source":["#Se define funcion para extrar las caracteristicas con la librería librosa, obetiendo los coeficientes ceptrales de frecuencia de Mel\n","#Se realiza un pading en el tamaño para que concuerden los tamaños de las caracteristicas de entrada al modelo.\n","\n","def extract_features(file_name, Nmfcc, Nfft, NhopL, NwinL):\n","\n","    samplerate = 22050\n","    longitudMaxAudio = 4\n","    max_pad_len = int(samplerate*longitudMaxAudio/NhopL) + int(samplerate*longitudMaxAudio/NhopL*0.05)  #Calculo longitud de salida de mfcc con 5% de tolerancia para longitud de audios\n","\n","    try:\n","      audio, sample_rate = librosa.load(file_name, res_type='soxr_hq')\n","      mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=Nmfcc, n_fft=Nfft, hop_length=NhopL, win_length=NwinL)\n","      pad_width = max_pad_len - mfccs.shape[1]\n","      mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n","\n","    except Exception as e:\n","      print(\"Error encountered while parsing file: \", file_name)\n","      return None\n","    #print(mfccs.shape)\n","    return mfccs"],"metadata":{"id":"IXHVBoz1Gydz","executionInfo":{"status":"ok","timestamp":1696396596667,"user_tz":300,"elapsed":24,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}}},"execution_count":178,"outputs":[]},{"cell_type":"code","source":["#Se realiza la extracción de caracteristicas, teniendo en cuenta la clase, si el sonido es de la carpeta agregada de la clase explosions va y busca este sonido en la carpeta requerida\n","\n","def get_features(Nmfcc, Nfft, NhopL, NwinL):\n","  features = []\n","  # Iterate through each sound file and extract the features\n","  for index, row in dfCompleteGunshot.iterrows():\n","      if(row[\"fold\"]!=0):\n","          file_name = os.path.join(os.path.abspath(file_path),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n","          #file_name = \"\"\n","      elif(row[\"fold\"]==0):\n","          file_name = os.path.join(os.path.abspath(explosions_path),str(row[\"slice_file_name\"]))\n","          #file_name = \"\"\n","      #print(file_name)\n","      class_label = row[\"classID\"]\n","      data = extract_features(file_name, Nmfcc, Nfft, NhopL, NwinL)\n","\n","      features.append([data, class_label])\n","\n","  # Convert into a Panda dataframe\n","  featuresdf = pd.DataFrame(features, columns=['feature','class_label'])\n","  return featuresdf"],"metadata":{"id":"lkKtNtY1G0mf","executionInfo":{"status":"ok","timestamp":1696396596667,"user_tz":300,"elapsed":24,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}}},"execution_count":179,"outputs":[]},{"cell_type":"code","source":["def splitFeaturesTrainTest(featuresdf, num_rows, num_columns, num_channels):\n","  X = np.array(featuresdf.feature.tolist())\n","  y = np.array(featuresdf.class_label.tolist())\n","\n","  # Encode the classification labels\n","  le = LabelEncoder()\n","  yy = to_categorical(le.fit_transform(y))\n","\n","  # split the dataset\n","  from sklearn.model_selection import train_test_split\n","\n","  x_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 3)\n","\n","  x_train = x_train.reshape(x_train.shape[0], num_rows, num_columns, num_channels)\n","  x_test = x_test.reshape(x_test.shape[0], num_rows, num_columns, num_channels)\n","  num_labels = yy.shape[1]\n","  return x_train, x_test, y_train, y_test, num_labels"],"metadata":{"id":"fWTmQgidG22t","executionInfo":{"status":"ok","timestamp":1696396596667,"user_tz":300,"elapsed":23,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}}},"execution_count":180,"outputs":[]},{"cell_type":"code","source":["\n","#1\t3\t4096\t4096\t4096\t0\t2\n","#2\t12\t4096\t4096\t4096\t0\t3\n","#3\t27\t4096\t4096\t4096\t0\t3\n","#4\t45\t4096\t4096\t4096\t0\t5\n","\n","samplerate = 22050\n","longitudMaxAudio = 4\n","Nmfcc = 3\n","Nfft = 4096\n","NwinL = 4096\n","iterableNhopL = 1.0\n","NhopL =  4096       #int(iterableNhopL*NwinL)\n","k_size = 2\n","num_rows = Nmfcc\n","num_columns = int(samplerate*longitudMaxAudio/NhopL) + int(samplerate*longitudMaxAudio/NhopL*0.05)  #Calculo longitud de salida de mfcc con 5% de tolerancia para longitud de audios\n","num_channels = 1\n","\n","\n","featuresdfGunshot = get_features(Nmfcc, Nfft, NhopL, NwinL)\n","x_trainGunshot, x_testGunshot, y_trainGunshot, y_testGunshot, num_labelsGunshot = splitFeaturesTrainTest(featuresdfGunshot, num_rows, num_columns, num_channels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b_U5gBB1G6Wu","executionInfo":{"status":"ok","timestamp":1696396623679,"user_tz":300,"elapsed":27035,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}},"outputId":"771c3f68-e24b-438c-dbc6-809dd40a76c8"},"execution_count":181,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/librosa/core/spectrum.py:257: UserWarning: n_fft=4096 is too large for input signal of length=3969\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/librosa/core/spectrum.py:257: UserWarning: n_fft=4096 is too large for input signal of length=3949\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/librosa/core/spectrum.py:257: UserWarning: n_fft=4096 is too large for input signal of length=3667\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["#PREPARE SIREN DATASET"],"metadata":{"id":"q5XZyZjRdKN6"}},{"cell_type":"code","source":["# Se obtienen las diferentes rutas de los datos, tanto audios como metadata y path para obtener las muestras\n","file_path = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/UrbanSounds8k/UrbanSound8K/audio'\n","urbansound8k = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/UrbanSounds8k/UrbanSound8K/metadata/UrbanSound8K.csv')\n","file_viz = glob.glob('/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/UrbanSounds8k/UrbanSound8K/audio/fold1/*')"],"metadata":{"id":"q_EgN2eFdMzK","executionInfo":{"status":"ok","timestamp":1696396623679,"user_tz":300,"elapsed":5,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}}},"execution_count":182,"outputs":[]},{"cell_type":"code","source":["#Se organiza dataset para seleccionar la clase de interes y las otras renombrarse como no interes\n","urbansound8k.loc[urbansound8k[\"class\"] != \"siren\", \"class\"] = \"non_siren\"\n","#Se cambian los valores de la columna classID para identificar unicamente las dos clases de interes\n","urbansound8k.loc[urbansound8k[\"classID\"] != 8, \"classID\"] = 0 #non_event\n","urbansound8k.loc[urbansound8k[\"classID\"] == 8, \"classID\"] = 1 #siren\n","\n","#se cuenta el numero total de muestras de la clase de interes\n","totalSamplesSiren =  urbansound8k.loc[((urbansound8k[\"class\"]==\"siren\"))].count()[0]\n","totalSamplesSiren\n","\n","#se toma una muestra aleatoria del tamaño de las muestras de la clase de interes para que quede balanceado\n","dfNonSiren = urbansound8k[urbansound8k['class'] == \"non_siren\"].sample(n=totalSamplesSiren)\n","dfSiren = urbansound8k[urbansound8k['class'] == \"siren\"]\n","\n","#se unen en un solo dataset las clases de interes con muestras balanceadas\n","dfCompleteSiren = dfNonSiren.append(dfSiren)\n","dfCompleteSiren.count()[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lir6LVEkdOtY","executionInfo":{"status":"ok","timestamp":1696396624255,"user_tz":300,"elapsed":8,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}},"outputId":"b3cd0687-769f-4859-d84d-e6adff2d4110"},"execution_count":183,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-183-34a8a78805fd>:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  dfCompleteSiren = dfNonSiren.append(dfSiren)\n"]},{"output_type":"execute_result","data":{"text/plain":["1858"]},"metadata":{},"execution_count":183}]},{"cell_type":"code","source":["#Se define funcion para extrar las caracteristicas con la librería librosa, obetiendo los coeficientes ceptrales de frecuencia de Mel\n","#Se realiza un pading en el tamaño para que concuerden los tamaños de las caracteristicas de entrada al modelo.\n","\n","def extract_features(file_name, Nmfcc, Nfft, NhopL, NwinL):\n","\n","    samplerate = 22050\n","    longitudMaxAudio = 4\n","    max_pad_len = int(samplerate*longitudMaxAudio/NhopL) + int(samplerate*longitudMaxAudio/NhopL*0.05)  #Calculo longitud de salida de mfcc con 5% de tolerancia para longitud de audios\n","\n","    try:\n","      audio, sample_rate = librosa.load(file_name, res_type='soxr_hq')\n","      mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=Nmfcc, n_fft=Nfft, hop_length=NhopL, win_length=NwinL)\n","      pad_width = max_pad_len - mfccs.shape[1]\n","      mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n","\n","    except Exception as e:\n","      print(\"Error encountered while parsing file: \", file_name)\n","      return None\n","    #print(mfccs.shape)\n","    return mfccs"],"metadata":{"id":"RcDQh2XedQiq","executionInfo":{"status":"ok","timestamp":1696396624256,"user_tz":300,"elapsed":6,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}}},"execution_count":184,"outputs":[]},{"cell_type":"code","source":["#Se realiza la extracción de caracteristicas, teniendo en cuenta la clase, si el sonido es de la carpeta agregada de la clase explosions va y busca este sonido en la carpeta requerida\n","\n","def get_features(Nmfcc, Nfft, NhopL, NwinL):\n","  features = []\n","  # Iterate through each sound file and extract the features\n","  for index, row in dfCompleteSiren.iterrows():\n","      if(row[\"fold\"]!=0):\n","          file_name = os.path.join(os.path.abspath(file_path),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n","          #file_name = \"\"\n","      elif(row[\"fold\"]==0):\n","          file_name = os.path.join(os.path.abspath(explosions_path),str(row[\"slice_file_name\"]))\n","          #file_name = \"\"\n","      #print(file_name)\n","      class_label = row[\"classID\"]\n","      data = extract_features(file_name, Nmfcc, Nfft, NhopL, NwinL)\n","\n","      features.append([data, class_label])\n","\n","  # Convert into a Panda dataframe\n","  featuresdf = pd.DataFrame(features, columns=['feature','class_label'])\n","  return featuresdf"],"metadata":{"id":"1yGQbnPGdT4M","executionInfo":{"status":"ok","timestamp":1696396624256,"user_tz":300,"elapsed":5,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}}},"execution_count":185,"outputs":[]},{"cell_type":"code","source":["def splitFeaturesTrainTest(featuresdf, num_rows, num_columns, num_channels):\n","  X = np.array(featuresdf.feature.tolist())\n","  y = np.array(featuresdf.class_label.tolist())\n","\n","  # Encode the classification labels\n","  le = LabelEncoder()\n","  yy = to_categorical(le.fit_transform(y))\n","\n","  # split the dataset\n","  from sklearn.model_selection import train_test_split\n","\n","  x_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 3)\n","\n","  x_train = x_train.reshape(x_train.shape[0], num_rows, num_columns, num_channels)\n","  x_test = x_test.reshape(x_test.shape[0], num_rows, num_columns, num_channels)\n","  num_labels = yy.shape[1]\n","  return x_train, x_test, y_train, y_test, num_labels"],"metadata":{"id":"7n9azkcKdUrb","executionInfo":{"status":"ok","timestamp":1696396624256,"user_tz":300,"elapsed":5,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}}},"execution_count":186,"outputs":[]},{"cell_type":"code","source":["\n","#1\t3\t4096\t4096\t4096\t0\t2\n","#2\t12\t4096\t4096\t4096\t0\t3\n","#3\t27\t4096\t4096\t4096\t0\t3\n","#4\t45\t4096\t4096\t4096\t0\t5\n","\n","samplerate = 22050\n","longitudMaxAudio = 4\n","Nmfcc = 3\n","Nfft = 4096\n","NwinL = 4096\n","iterableNhopL = 1.0\n","NhopL =  4096       #int(iterableNhopL*NwinL)\n","k_size = 2\n","num_rows = Nmfcc\n","num_columns = int(samplerate*longitudMaxAudio/NhopL) + int(samplerate*longitudMaxAudio/NhopL*0.05)  #Calculo longitud de salida de mfcc con 5% de tolerancia para longitud de audios\n","num_channels = 1\n","\n","\n","featuresdfSiren = get_features(Nmfcc, Nfft, NhopL, NwinL)\n","x_trainSiren, x_testSiren, y_trainSiren, y_testSiren, num_labelsSiren = splitFeaturesTrainTest(featuresdfSiren, num_rows, num_columns, num_channels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WJgOkM-fdWY5","executionInfo":{"status":"ok","timestamp":1696396687169,"user_tz":300,"elapsed":62918,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}},"outputId":"4dafa7ef-e25f-4cf2-ebce-a6a401b3bc4e"},"execution_count":187,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/librosa/core/spectrum.py:257: UserWarning: n_fft=4096 is too large for input signal of length=3954\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/librosa/core/spectrum.py:257: UserWarning: n_fft=4096 is too large for input signal of length=1323\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/librosa/core/spectrum.py:257: UserWarning: n_fft=4096 is too large for input signal of length=3087\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/librosa/core/spectrum.py:257: UserWarning: n_fft=4096 is too large for input signal of length=3949\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["x_trainSiren.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4vne142vdYcB","executionInfo":{"status":"ok","timestamp":1696396687169,"user_tz":300,"elapsed":28,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}},"outputId":"72d40801-b7cd-4db9-9452-175d09399bbe"},"execution_count":188,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1486, 3, 22, 1)"]},"metadata":{},"execution_count":188}]},{"cell_type":"code","source":["num_labelsSiren"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B_JsCnoqdaY9","executionInfo":{"status":"ok","timestamp":1696396687169,"user_tz":300,"elapsed":25,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}},"outputId":"32be275e-47a9-4c1d-cadf-9107cfb89e6b"},"execution_count":189,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":189}]},{"cell_type":"code","source":["num_labelsScream"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b0KcpP-pdbqW","executionInfo":{"status":"ok","timestamp":1696396687169,"user_tz":300,"elapsed":22,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}},"outputId":"792aa4b0-f889-4527-b597-b3d7dc9ef441"},"execution_count":190,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":190}]},{"cell_type":"code","source":["num_labelsGunshot"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HxUDzIrsddAh","executionInfo":{"status":"ok","timestamp":1696396687170,"user_tz":300,"elapsed":21,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}},"outputId":"9f068954-1148-42f4-c2c0-df0ca005d22d"},"execution_count":191,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":191}]},{"cell_type":"markdown","source":["#EVALUATING OG MODELS ARQ 1"],"metadata":{"id":"cFnSHOlgdk92"}},{"cell_type":"code","source":["modelGunshotTLSave_path = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/Models/Fusion/TL_Complete/gunshot/saved_gunshot_TL_1'\n","modelGunshotTL = tf.keras.models.load_model(modelGunshotTLSave_path)\n","\n","modelScreamTLSave_path = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/Models/Fusion/TL_Complete/scream/saved_scream_TL_1'\n","modelScreamTL = tf.keras.models.load_model(modelScreamTLSave_path)\n","\n","modelSirenTLSave_path = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/Models/Fusion/TL_Complete/siren/saved_siren_TL_1'\n","modelSirenTL = tf.keras.models.load_model(modelSirenTLSave_path)\n","modelSirenTL.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')\n","\n","\n","# Evaluating the model on the training and testing set\n","\n","score = modelScreamTL.evaluate(x_trainScream, y_trainScream, verbose=0)\n","print(\"Training Accuracy SCREAM: \", score[1])\n","\n","score = modelScreamTL.evaluate(x_testScream, y_testScream, verbose=0)\n","print(\"Testing Accuracy SCREAM: \", score[1])\n","\n","y_true = np.argmax(y_testScream,axis=1)\n","y_pred = np.argmax(modelScreamTL.predict(x_testScream),axis=1)\n","print('\\nConfusion Matrix SCREAM:\\n\\n')\n","print(confusion_matrix(y_true,y_pred))\n","print('\\n\\nClassification Report SCREAM: \\n\\n',classification_report(y_true,y_pred))\n","\n","score = modelGunshotTL.evaluate(x_trainGunshot, y_trainGunshot, verbose=0)\n","print(\"Training Accuracy GUN: \", score[1])\n","\n","score = modelGunshotTL.evaluate(x_testGunshot, y_testGunshot, verbose=0)\n","print(\"Testing Accuracy GUN: \", score[1])\n","\n","y_true = np.argmax(y_testGunshot,axis=1)\n","y_pred = np.argmax(modelGunshotTL.predict(x_testGunshot),axis=1)\n","print('\\nConfusion Matrix GUN:\\n\\n')\n","print(confusion_matrix(y_true,y_pred))\n","print('\\n\\nClassification Report GUN: \\n\\n',classification_report(y_true,y_pred))\n","print('\\n\\nClassification Report GUN F1-SCORE: \\n\\n', float(classification_report(y_true,y_pred).split()[15]))\n","\n","\n","score = modelSirenTL.evaluate(x_trainSiren, y_trainSiren, verbose=0)\n","print(\"Training Accuracy SIREN: \", score[1])\n","\n","score = modelSirenTL.evaluate(x_testSiren, y_testSiren, verbose=0)\n","print(\"Testing Accuracy SIREN: \", score[1])\n","\n","y_true = np.argmax(y_testSiren,axis=1)\n","y_pred = np.argmax(modelSirenTL.predict(x_testSiren),axis=1)\n","print('\\nConfusion Matrix SIREN:\\n\\n')\n","print(confusion_matrix(y_true,y_pred))\n","print('\\n\\nClassification Report SIREN: \\n\\n',classification_report(y_true,y_pred))\n","print('\\n\\nClassification Report SIREN F1-SCORE: \\n\\n', float(classification_report(y_true,y_pred).split()[15]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7nsdOE_2dlUl","executionInfo":{"status":"ok","timestamp":1696396692584,"user_tz":300,"elapsed":5433,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}},"outputId":"76c5bdec-9696-4dba-c1ad-f828f30e2ced"},"execution_count":192,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Accuracy SCREAM:  0.9936000108718872\n","Testing Accuracy SCREAM:  0.987261176109314\n","5/5 [==============================] - 0s 4ms/step\n","\n","Confusion Matrix SCREAM:\n","\n","\n","[[83  1]\n"," [ 1 72]]\n","\n","\n","Classification Report SCREAM: \n","\n","               precision    recall  f1-score   support\n","\n","           0       0.99      0.99      0.99        84\n","           1       0.99      0.99      0.99        73\n","\n","    accuracy                           0.99       157\n","   macro avg       0.99      0.99      0.99       157\n","weighted avg       0.99      0.99      0.99       157\n","\n","Training Accuracy GUN:  0.9515050053596497\n","Testing Accuracy GUN:  0.95333331823349\n","5/5 [==============================] - 0s 3ms/step\n","\n","Confusion Matrix GUN:\n","\n","\n","[[73  7]\n"," [ 0 70]]\n","\n","\n","Classification Report GUN: \n","\n","               precision    recall  f1-score   support\n","\n","           0       1.00      0.91      0.95        80\n","           1       0.91      1.00      0.95        70\n","\n","    accuracy                           0.95       150\n","   macro avg       0.95      0.96      0.95       150\n","weighted avg       0.96      0.95      0.95       150\n","\n","\n","\n","Classification Report GUN F1-SCORE: \n","\n"," 0.95\n","Training Accuracy SIREN:  0.8270524740219116\n","Testing Accuracy SIREN:  0.8172042965888977\n","12/12 [==============================] - 0s 3ms/step\n","\n","Confusion Matrix SIREN:\n","\n","\n","[[154  38]\n"," [ 30 150]]\n","\n","\n","Classification Report SIREN: \n","\n","               precision    recall  f1-score   support\n","\n","           0       0.84      0.80      0.82       192\n","           1       0.80      0.83      0.82       180\n","\n","    accuracy                           0.82       372\n","   macro avg       0.82      0.82      0.82       372\n","weighted avg       0.82      0.82      0.82       372\n","\n","\n","\n","Classification Report SIREN F1-SCORE: \n","\n"," 0.82\n"]}]},{"cell_type":"markdown","source":["#EVALUATING OG MODELS ARQ 4"],"metadata":{"id":"9xUabloD3HtZ"}},{"cell_type":"code","source":["modelGunshotTLSave_path = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/Models/Fusion/TL_Complete/gunshot/saved_gunshot_TL_4'\n","modelGunshotTL = tf.keras.models.load_model(modelGunshotTLSave_path)\n","\n","modelScreamTLSave_path = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/Models/Fusion/TL_Complete/scream/saved_scream_TL_4'\n","modelScreamTL = tf.keras.models.load_model(modelScreamTLSave_path)\n","\n","modelSirenTLSave_path = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/Models/Fusion/TL_Complete/siren/saved_siren_TL_4'\n","modelSirenTL = tf.keras.models.load_model(modelSirenTLSave_path)\n","modelSirenTL.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')\n","\n","\n","# Evaluating the model on the training and testing set\n","\n","score = modelScreamTL.evaluate(x_trainScream, y_trainScream, verbose=0)\n","print(\"Training Accuracy SCREAM: \", score[1])\n","\n","score = modelScreamTL.evaluate(x_testScream, y_testScream, verbose=0)\n","print(\"Testing Accuracy SCREAM: \", score[1])\n","\n","y_true = np.argmax(y_testScream,axis=1)\n","y_pred = np.argmax(modelScreamTL.predict(x_testScream),axis=1)\n","print('\\nConfusion Matrix SCREAM:\\n\\n')\n","print(confusion_matrix(y_true,y_pred))\n","print('\\n\\nClassification Report SCREAM: \\n\\n',classification_report(y_true,y_pred))\n","\n","score = modelGunshotTL.evaluate(x_trainGunshot, y_trainGunshot, verbose=0)\n","print(\"Training Accuracy GUN: \", score[1])\n","\n","score = modelGunshotTL.evaluate(x_testGunshot, y_testGunshot, verbose=0)\n","print(\"Testing Accuracy GUN: \", score[1])\n","\n","y_true = np.argmax(y_testGunshot,axis=1)\n","y_pred = np.argmax(modelGunshotTL.predict(x_testGunshot),axis=1)\n","print('\\nConfusion Matrix GUN:\\n\\n')\n","print(confusion_matrix(y_true,y_pred))\n","print('\\n\\nClassification Report GUN: \\n\\n',classification_report(y_true,y_pred))\n","print('\\n\\nClassification Report GUN F1-SCORE: \\n\\n', float(classification_report(y_true,y_pred).split()[15]))\n","\n","\n","score = modelSirenTL.evaluate(x_trainSiren, y_trainSiren, verbose=0)\n","print(\"Training Accuracy SIREN: \", score[1])\n","\n","score = modelSirenTL.evaluate(x_testSiren, y_testSiren, verbose=0)\n","print(\"Testing Accuracy SIREN: \", score[1])\n","\n","y_true = np.argmax(y_testSiren,axis=1)\n","y_pred = np.argmax(modelSirenTL.predict(x_testSiren),axis=1)\n","print('\\nConfusion Matrix SIREN:\\n\\n')\n","print(confusion_matrix(y_true,y_pred))\n","print('\\n\\nClassification Report SIREN: \\n\\n',classification_report(y_true,y_pred))\n","print('\\n\\nClassification Report SIREN F1-SCORE: \\n\\n', float(classification_report(y_true,y_pred).split()[15]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3amdah5h3HGH","executionInfo":{"status":"ok","timestamp":1696392537641,"user_tz":300,"elapsed":10468,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}},"outputId":"d8afcac4-1593-41e1-8ea7-3a4172856586"},"execution_count":118,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Accuracy SCREAM:  1.0\n","Testing Accuracy SCREAM:  0.987261176109314\n","5/5 [==============================] - 0s 24ms/step\n","\n","Confusion Matrix SCREAM:\n","\n","\n","[[83  1]\n"," [ 1 72]]\n","\n","\n","Classification Report SCREAM: \n","\n","               precision    recall  f1-score   support\n","\n","           0       0.99      0.99      0.99        84\n","           1       0.99      0.99      0.99        73\n","\n","    accuracy                           0.99       157\n","   macro avg       0.99      0.99      0.99       157\n","weighted avg       0.99      0.99      0.99       157\n","\n","Training Accuracy GUN:  0.9765886068344116\n","Testing Accuracy GUN:  0.9866666793823242\n","5/5 [==============================] - 0s 21ms/step\n","\n","Confusion Matrix GUN:\n","\n","\n","[[79  1]\n"," [ 1 69]]\n","\n","\n","Classification Report GUN: \n","\n","               precision    recall  f1-score   support\n","\n","           0       0.99      0.99      0.99        80\n","           1       0.99      0.99      0.99        70\n","\n","    accuracy                           0.99       150\n","   macro avg       0.99      0.99      0.99       150\n","weighted avg       0.99      0.99      0.99       150\n","\n","\n","\n","Classification Report GUN F1-SCORE: \n","\n"," 0.99\n","Training Accuracy SIREN:  0.9919246435165405\n","Testing Accuracy SIREN:  0.9731183052062988\n","12/12 [==============================] - 0s 22ms/step\n","\n","Confusion Matrix SIREN:\n","\n","\n","[[184   8]\n"," [  2 178]]\n","\n","\n","Classification Report SIREN: \n","\n","               precision    recall  f1-score   support\n","\n","           0       0.99      0.96      0.97       192\n","           1       0.96      0.99      0.97       180\n","\n","    accuracy                           0.97       372\n","   macro avg       0.97      0.97      0.97       372\n","weighted avg       0.97      0.97      0.97       372\n","\n","\n","\n","Classification Report SIREN F1-SCORE: \n","\n"," 0.97\n"]}]},{"cell_type":"markdown","source":["#Fusion Gun Filters into Scream Filters model"],"metadata":{"id":"0WjtNlJSZTqR"}},{"cell_type":"code","source":["def get_conv_layer(model, layer_index):\n","    # Find the first convolutional layer starting from the given layer index\n","    for layer in model.layers[layer_index:]:\n","        if 'conv' in layer.name:\n","            return layer\n","    raise ValueError(\"No convolutional layer found starting from the given layer index.\")\n","\n","def get_conv_layer(model, layer_index):\n","    conv_layers = [layer for layer in model.layers if 'conv' in layer.name]\n","\n","    if layer_index < 0 or layer_index >= len(conv_layers):\n","        raise ValueError(\"Invalid layer index.\")\n","\n","    return conv_layers[layer_index]\n","\n","\n","def change_filters(model1, model2, layer_index):\n","    layer1 = get_conv_layer(model1, layer_index)\n","    filters1, bias1 = layer1.get_weights()\n","\n","    layer2 = get_conv_layer(model2, layer_index)\n","    filters2, bias2 = layer2.get_weights()\n","    layer2.set_weights([filters1, bias1])\n","    layer1.set_weights([filters2, bias2])\n"],"metadata":{"id":"OH0aBo-iYWfl","executionInfo":{"status":"ok","timestamp":1696396734104,"user_tz":300,"elapsed":409,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}}},"execution_count":193,"outputs":[]},{"cell_type":"code","source":["def get_conv_layer(model, layer_index):\n","    conv_layers = [layer for layer in model.layers if 'conv' in layer.name]\n","\n","    if layer_index < 0 or layer_index >= len(conv_layers):\n","        raise ValueError(\"Invalid layer index.\")\n","\n","    return conv_layers[layer_index]\n","\n","\n","\n","def change_filters(model1, model2, layer_index, transfer_direction):\n","    layer1 = get_conv_layer(model1, layer_index)\n","    filters1, bias1 = layer1.get_weights()\n","\n","    layer2 = get_conv_layer(model2, layer_index)\n","    filters2, bias2 = layer2.get_weights()\n","\n","    if transfer_direction == \"model1_to_model2\":\n","        layer2.set_weights([filters1, bias1])\n","    elif transfer_direction == \"model2_to_model1\":\n","        layer1.set_weights([filters2, bias2])\n","    elif transfer_direction == \"both_directions\":\n","        layer2.set_weights([filters1, bias1])\n","        layer1.set_weights([filters2, bias2])\n","    else:\n","        raise ValueError(\"Invalid transfer direction.\")\n","\n","# Ejemplos de uso:\n","# Cambiar los filtros del modelo 1 al modelo 2\n","#change_filters(model1, model2, 0, \"model1_to_model2\")\n","\n","# Cambiar los filtros del modelo 2 al modelo 1\n","#change_filters(model1, model2, 0, \"model2_to_model1\")\n","\n","# Intercambiar los filtros en ambas direcciones\n","#change_filters(model1, model2, 0, \"both_directions\")"],"metadata":{"id":"RPsbnLjvSNeT","executionInfo":{"status":"ok","timestamp":1696396735353,"user_tz":300,"elapsed":4,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}}},"execution_count":194,"outputs":[]},{"cell_type":"markdown","source":["#Evaluation model with changes without Criterium"],"metadata":{"id":"gVPYPkFCY6pu"}},{"cell_type":"markdown","source":["#Evaluation model with changes with GUN and SCREAM MODELS BOTH FROM TL PROGRESSIVE LAYER BY LAYER"],"metadata":{"id":"1n2Ff4_vao_U"}},{"cell_type":"markdown","source":["#GUN VS SCREAM ARQ 1"],"metadata":{"id":"nG_psmj0tD-l"}},{"cell_type":"code","source":["\n","modelGunshotTLSave_path_1 = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/Models/Fusion/TL_Complete/gunshot/saved_gunshot_TL_1'\n","modelGunshotTL_1 = tf.keras.models.load_model(modelGunshotTLSave_path_1)\n","\n","modelScreamTLSave_path_1 = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/Models/Fusion/TL_Complete/scream/saved_scream_TL_1'\n","modelScreamTL_1 = tf.keras.models.load_model(modelScreamTLSave_path_1)\n","\n","modelSirenTLSave_path_1 = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/Models/Fusion/TL_Complete/siren/saved_siren_TL_1'\n","modelSirenTL_1 = tf.keras.models.load_model(modelSirenTLSave_path_1)\n","\n","\n","#GUN VS SCREAM\n","layer_index = 3  # Specify the index of the layer (e.g., 0 for the first convolutional layer)\n","change_filters(modelGunshotTL_1, modelScreamTL_1, 0, \"model1_to_model2\")\n","#change_filters(modelGunshotTL_1, modelScreamTL_1, 0, \"model2_to_model1\")\n","\n","#change_filters(modelGunshotTL_1, modelScreamTL_1, 1, \"model1_to_model2\")\n","change_filters(modelGunshotTL_1, modelScreamTL_1, 1, \"model2_to_model1\")\n","\n","#change_filters(modelGunshotTL_1, modelScreamTL_1, 2, \"model1_to_model2\")\n","change_filters(modelGunshotTL_1, modelScreamTL_1, 2, \"model2_to_model1\")\n","\n","change_filters(modelGunshotTL_1, modelScreamTL_1, 3, \"model1_to_model2\")\n","#change_filters(modelGunshotTL_1, modelScreamTL_1, 3, \"model2_to_model1\")\n","\n","\n","\n","# Evaluating the model on the training and testing set\n","print(\"GUNSHOT: \")\n","score = modelGunshotTL_1.evaluate(x_trainGunshot, y_trainGunshot, verbose=0)\n","print(\"Training Accuracy: \", score[1])\n","\n","score = modelGunshotTL_1.evaluate(x_testGunshot, y_testGunshot, verbose=0)\n","print(\"Testing Accuracy: \", score[1])\n","\n","y_true = np.argmax(y_testGunshot,axis=1)\n","y_pred = np.argmax(modelGunshotTL_1.predict(x_testGunshot),axis=1)\n","print('\\nConfusion Matrix :\\n\\n')\n","print(confusion_matrix(y_true,y_pred))\n","print('\\n\\nClassification Report : \\n\\n',classification_report(y_true,y_pred))\n","\n","\n","print(\"SCREAM: \")\n","score = modelScreamTL_1.evaluate(x_trainScream, y_trainScream, verbose=0)\n","print(\"Training Accuracy: \", score[1])\n","\n","score = modelScreamTL_1.evaluate(x_testScream, y_testScream, verbose=0)\n","print(\"Testing Accuracy: \", score[1])\n","\n","y_true = np.argmax(y_testScream,axis=1)\n","y_pred = np.argmax(modelScreamTL_1.predict(x_testScream),axis=1)\n","print('\\nConfusion Matrix :\\n\\n')\n","print(confusion_matrix(y_true,y_pred))\n","print('\\n\\nClassification Report : \\n\\n',classification_report(y_true,y_pred))\n","\n"],"metadata":{"id":"xVXZ_4RyarOL","executionInfo":{"status":"ok","timestamp":1696389422854,"user_tz":300,"elapsed":11898,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}}},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":["#GUN VS SIREN ARQ 1"],"metadata":{"id":"ido92UI3tOsL"}},{"cell_type":"code","source":["modelGunshotTLSave_path_1 = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/Models/Fusion/TL_Complete/gunshot/saved_gunshot_TL_1'\n","modelGunshotTL_1 = tf.keras.models.load_model(modelGunshotTLSave_path_1)\n","\n","modelScreamTLSave_path_1 = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/Models/Fusion/TL_Complete/scream/saved_scream_TL_1'\n","modelScreamTL_1 = tf.keras.models.load_model(modelScreamTLSave_path_1)\n","\n","modelSirenTLSave_path_1 = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/Models/Fusion/TL_Complete/siren/saved_siren_TL_1'\n","modelSirenTL_1 = tf.keras.models.load_model(modelSirenTLSave_path_1)\n","\n","\n","#GUN VS SIREN\n","#layer_index = 3  # Specify the index of the layer (e.g., 0 for the first convolutional layer)\n","\n","#change_filters(modelGunshotTL_1, modelSirenTL_1, 0, \"model1_to_model2\")\n","change_filters(modelGunshotTL_1, modelSirenTL_1, 0, \"model2_to_model1\")\n","\n","#change_filters(modelGunshotTL_1, modelSirenTL_1, 1, \"model1_to_model2\")\n","change_filters(modelGunshotTL_1, modelSirenTL_1, 1, \"model2_to_model1\")\n","\n","#change_filters(modelGunshotTL_1, modelSirenTL_1, 2, \"model1_to_model2\")\n","change_filters(modelGunshotTL_1, modelSirenTL_1, 2, \"model2_to_model1\")\n","\n","change_filters(modelGunshotTL_1, modelSirenTL_1, 3, \"model1_to_model2\")\n","#change_filters(modelGunshotTL_1, modelSirenTL_1, 3, \"model2_to_model1\")\n","\n","\n","\n","# Evaluating the model on the training and testing set\n","print(\"GUNSHOT: \")\n","score = modelGunshotTL_1.evaluate(x_trainGunshot, y_trainGunshot, verbose=0)\n","print(\"Training Accuracy: \", score[1])\n","\n","score = modelGunshotTL_1.evaluate(x_testGunshot, y_testGunshot, verbose=0)\n","print(\"Testing Accuracy: \", score[1])\n","\n","y_true = np.argmax(y_testGunshot,axis=1)\n","y_pred = np.argmax(modelGunshotTL_1.predict(x_testGunshot),axis=1)\n","print('\\nConfusion Matrix :\\n\\n')\n","print(confusion_matrix(y_true,y_pred))\n","print('\\n\\nClassification Report : \\n\\n',classification_report(y_true,y_pred))\n","\n","\n","print(\"SIREN: \")\n","score = modelSirenTL_1.evaluate(x_trainSiren, y_trainSiren, verbose=0)\n","print(\"Training Accuracy: \", score[1])\n","\n","score = modelSirenTL_1.evaluate(x_testSiren, y_testSiren, verbose=0)\n","print(\"Testing Accuracy: \", score[1])\n","\n","y_true = np.argmax(y_testSiren,axis=1)\n","y_pred = np.argmax(modelSirenTL_1.predict(x_testSiren),axis=1)\n","print('\\nConfusion Matrix :\\n\\n')\n","print(confusion_matrix(y_true,y_pred))\n","print('\\n\\nClassification Report : \\n\\n',classification_report(y_true,y_pred))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7G6QfNr2tQOf","executionInfo":{"status":"ok","timestamp":1696390634277,"user_tz":300,"elapsed":8931,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}},"outputId":"062c3d09-9795-4a26-f788-25c1bd738664"},"execution_count":64,"outputs":[{"output_type":"stream","name":"stdout","text":["GUNSHOT: \n","Training Accuracy:  0.7006688714027405\n","Testing Accuracy:  0.6733333468437195\n","5/5 [==============================] - 0s 4ms/step\n","\n","Confusion Matrix :\n","\n","\n","[[37 43]\n"," [ 6 64]]\n","\n","\n","Classification Report : \n","\n","               precision    recall  f1-score   support\n","\n","           0       0.86      0.46      0.60        80\n","           1       0.60      0.91      0.72        70\n","\n","    accuracy                           0.67       150\n","   macro avg       0.73      0.69      0.66       150\n","weighted avg       0.74      0.67      0.66       150\n","\n","SIREN: \n","Training Accuracy:  0.8270524740219116\n","Testing Accuracy:  0.8172042965888977\n","12/12 [==============================] - 0s 4ms/step\n","\n","Confusion Matrix :\n","\n","\n","[[154  38]\n"," [ 30 150]]\n","\n","\n","Classification Report : \n","\n","               precision    recall  f1-score   support\n","\n","           0       0.84      0.80      0.82       192\n","           1       0.80      0.83      0.82       180\n","\n","    accuracy                           0.82       372\n","   macro avg       0.82      0.82      0.82       372\n","weighted avg       0.82      0.82      0.82       372\n","\n"]}]},{"cell_type":"markdown","source":["#SCREAM VS SIREN ARQ 1"],"metadata":{"id":"spHC1-eXthaa"}},{"cell_type":"code","source":["modelGunshotTLSave_path_1 = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/Models/Fusion/TL_Complete/gunshot/saved_gunshot_TL_1'\n","modelGunshotTL_1 = tf.keras.models.load_model(modelGunshotTLSave_path_1)\n","\n","modelScreamTLSave_path_1 = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/Models/Fusion/TL_Complete/scream/saved_scream_TL_1'\n","modelScreamTL_1 = tf.keras.models.load_model(modelScreamTLSave_path_1)\n","\n","modelSirenTLSave_path_1 = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/Models/Fusion/TL_Complete/siren/saved_siren_TL_1'\n","modelSirenTL_1 = tf.keras.models.load_model(modelSirenTLSave_path_1)\n","\n","\n","#SCREAM VS SIREN\n","#layer_index = 3  # Specify the index of the layer (e.g., 0 for the first convolutional layer)\n","\n","#change_filters(modelScreamTL_1, modelSirenTL_1, 0, \"model1_to_model2\")\n","change_filters(modelScreamTL_1, modelSirenTL_1, 0, \"model2_to_model1\")\n","\n","#change_filters(modelScreamTL_1, modelSirenTL_1, 1, \"model1_to_model2\")\n","change_filters(modelScreamTL_1, modelSirenTL_1, 1, \"model2_to_model1\")\n","\n","#change_filters(modelScreamTL_1, modelSirenTL_1, 2, \"model1_to_model2\")\n","change_filters(modelScreamTL_1, modelSirenTL_1, 2, \"model2_to_model1\")\n","\n","#change_filters(modelScreamTL_1, modelSirenTL_1, 3, \"model1_to_model2\")\n","change_filters(modelScreamTL_1, modelSirenTL_1, 3, \"model2_to_model1\")\n","\n","\n","\n","# Evaluating the model on the training and testing set\n","print(\"SCREAM: \")\n","score = modelScreamTL_1.evaluate(x_trainScream, y_trainScream, verbose=0)\n","print(\"Training Accuracy: \", score[1])\n","\n","score = modelScreamTL_1.evaluate(x_testScream, y_testScream, verbose=0)\n","print(\"Testing Accuracy: \", score[1])\n","\n","y_true = np.argmax(y_testScream,axis=1)\n","y_pred = np.argmax(modelScreamTL_1.predict(x_testScream),axis=1)\n","print('\\nConfusion Matrix :\\n\\n')\n","print(confusion_matrix(y_true,y_pred))\n","print('\\n\\nClassification Report : \\n\\n',classification_report(y_true,y_pred))\n","\n","\n","print(\"SIREN: \")\n","score = modelSirenTL_1.evaluate(x_trainSiren, y_trainSiren, verbose=0)\n","print(\"Training Accuracy: \", score[1])\n","\n","score = modelSirenTL_1.evaluate(x_testSiren, y_testSiren, verbose=0)\n","print(\"Testing Accuracy: \", score[1])\n","\n","y_true = np.argmax(y_testSiren,axis=1)\n","y_pred = np.argmax(modelSirenTL_1.predict(x_testSiren),axis=1)\n","print('\\nConfusion Matrix :\\n\\n')\n","print(confusion_matrix(y_true,y_pred))\n","print('\\n\\nClassification Report : \\n\\n',classification_report(y_true,y_pred))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GpVjhV97tj4f","executionInfo":{"status":"ok","timestamp":1696391316505,"user_tz":300,"elapsed":13928,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}},"outputId":"33090c0a-4de9-4212-8bc4-8ddc9a91f76c"},"execution_count":72,"outputs":[{"output_type":"stream","name":"stdout","text":["SCREAM: \n","Training Accuracy:  0.795199990272522\n","Testing Accuracy:  0.808917224407196\n","5/5 [==============================] - 0s 6ms/step\n","\n","Confusion Matrix :\n","\n","\n","[[79  5]\n"," [25 48]]\n","\n","\n","Classification Report : \n","\n","               precision    recall  f1-score   support\n","\n","           0       0.76      0.94      0.84        84\n","           1       0.91      0.66      0.76        73\n","\n","    accuracy                           0.81       157\n","   macro avg       0.83      0.80      0.80       157\n","weighted avg       0.83      0.81      0.80       157\n","\n","SIREN: \n","Training Accuracy:  0.8270524740219116\n","Testing Accuracy:  0.8172042965888977\n","12/12 [==============================] - 0s 4ms/step\n","\n","Confusion Matrix :\n","\n","\n","[[154  38]\n"," [ 30 150]]\n","\n","\n","Classification Report : \n","\n","               precision    recall  f1-score   support\n","\n","           0       0.84      0.80      0.82       192\n","           1       0.80      0.83      0.82       180\n","\n","    accuracy                           0.82       372\n","   macro avg       0.82      0.82      0.82       372\n","weighted avg       0.82      0.82      0.82       372\n","\n"]}]},{"cell_type":"markdown","source":["#GUN VS SCREAM ARQ 4"],"metadata":{"id":"xsQhEYJp5V4-"}},{"cell_type":"code","source":["\n","modelGunshotTLSave_path_4 = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/Models/Fusion/TL_Complete/gunshot/saved_gunshot_TL_4'\n","modelGunshotTL_4 = tf.keras.models.load_model(modelGunshotTLSave_path_4)\n","\n","modelScreamTLSave_path_4 = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/Models/Fusion/TL_Complete/scream/saved_scream_TL_4'\n","modelScreamTL_4 = tf.keras.models.load_model(modelScreamTLSave_path_4)\n","\n","modelSirenTLSave_path_4 = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/Models/Fusion/TL_Complete/siren/saved_siren_TL_4'\n","modelSirenTL_4 = tf.keras.models.load_model(modelSirenTLSave_path_4)\n","\n","\n","#GUN VS SCREAM\n","layer_index = 3  # Specify the index of the layer (e.g., 0 for the first convolutional layer)\n","\n","change_filters(modelGunshotTL_4, modelScreamTL_4, 0, \"model1_to_model2\")\n","#change_filters(modelGunshotTL_4, modelScreamTL_4, 0, \"model2_to_model1\")\n","\n","change_filters(modelGunshotTL_4, modelScreamTL_4, 1, \"model1_to_model2\")\n","#change_filters(modelGunshotTL_4, modelScreamTL_4, 1, \"model2_to_model1\")\n","\n","change_filters(modelGunshotTL_4, modelScreamTL_4, 2, \"model1_to_model2\")\n","#change_filters(modelGunshotTL_4, modelScreamTL_4, 2, \"model2_to_model1\")\n","\n","#change_filters(modelGunshotTL_4, modelScreamTL_4, 3, \"model1_to_model2\")\n","change_filters(modelGunshotTL_4, modelScreamTL_4, 3, \"model2_to_model1\")\n","\n","\n","\n","# Evaluating the model on the training and testing set\n","print(\"GUNSHOT: \")\n","score = modelGunshotTL_4.evaluate(x_trainGunshot, y_trainGunshot, verbose=0)\n","print(\"Training Accuracy: \", score[1])\n","\n","score = modelGunshotTL_4.evaluate(x_testGunshot, y_testGunshot, verbose=0)\n","print(\"Testing Accuracy: \", score[1])\n","\n","y_true = np.argmax(y_testGunshot,axis=1)\n","y_pred = np.argmax(modelGunshotTL_4.predict(x_testGunshot),axis=1)\n","print('\\nConfusion Matrix :\\n\\n')\n","print(confusion_matrix(y_true,y_pred))\n","print('\\n\\nClassification Report : \\n\\n',classification_report(y_true,y_pred))\n","\n","\n","print(\"SCREAM: \")\n","score = modelScreamTL_4.evaluate(x_trainScream, y_trainScream, verbose=0)\n","print(\"Training Accuracy: \", score[1])\n","\n","score = modelScreamTL_4.evaluate(x_testScream, y_testScream, verbose=0)\n","print(\"Testing Accuracy: \", score[1])\n","\n","y_true = np.argmax(y_testScream,axis=1)\n","y_pred = np.argmax(modelScreamTL_4.predict(x_testScream),axis=1)\n","print('\\nConfusion Matrix :\\n\\n')\n","print(confusion_matrix(y_true,y_pred))\n","print('\\n\\nClassification Report : \\n\\n',classification_report(y_true,y_pred))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i1-2-0z45WPy","executionInfo":{"status":"ok","timestamp":1696393992391,"user_tz":300,"elapsed":9648,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}},"outputId":"83f069b9-1dad-40d3-9760-9a2c44b66f00"},"execution_count":131,"outputs":[{"output_type":"stream","name":"stdout","text":["GUNSHOT: \n","Training Accuracy:  0.8127090334892273\n","Testing Accuracy:  0.8133333325386047\n","5/5 [==============================] - 0s 22ms/step\n","\n","Confusion Matrix :\n","\n","\n","[[55 25]\n"," [ 3 67]]\n","\n","\n","Classification Report : \n","\n","               precision    recall  f1-score   support\n","\n","           0       0.95      0.69      0.80        80\n","           1       0.73      0.96      0.83        70\n","\n","    accuracy                           0.81       150\n","   macro avg       0.84      0.82      0.81       150\n","weighted avg       0.85      0.81      0.81       150\n","\n","SCREAM: \n","Training Accuracy:  0.9552000164985657\n","Testing Accuracy:  0.9617834687232971\n","5/5 [==============================] - 0s 23ms/step\n","\n","Confusion Matrix :\n","\n","\n","[[80  4]\n"," [ 2 71]]\n","\n","\n","Classification Report : \n","\n","               precision    recall  f1-score   support\n","\n","           0       0.98      0.95      0.96        84\n","           1       0.95      0.97      0.96        73\n","\n","    accuracy                           0.96       157\n","   macro avg       0.96      0.96      0.96       157\n","weighted avg       0.96      0.96      0.96       157\n","\n"]}]},{"cell_type":"markdown","source":["#GUN VS SIREN ARQ 4"],"metadata":{"id":"IEvv1ASk5uGr"}},{"cell_type":"code","source":["\n","modelGunshotTLSave_path_4 = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/Models/Fusion/TL_Complete/gunshot/saved_gunshot_TL_4'\n","modelGunshotTL_4 = tf.keras.models.load_model(modelGunshotTLSave_path_4)\n","\n","modelScreamTLSave_path_4 = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/Models/Fusion/TL_Complete/scream/saved_scream_TL_4'\n","modelScreamTL_4 = tf.keras.models.load_model(modelScreamTLSave_path_4)\n","\n","modelSirenTLSave_path_4 = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/Models/Fusion/TL_Complete/siren/saved_siren_TL_4'\n","modelSirenTL_4 = tf.keras.models.load_model(modelSirenTLSave_path_4)\n","\n","\n","#GUN VS SIREN\n","#layer_index = 3  # Specify the index of the layer (e.g., 0 for the first convolutional layer)\n","\n","#change_filters(modelGunshotTL_4, modelSirenTL_4, 0, \"model1_to_model2\")\n","change_filters(modelGunshotTL_4, modelSirenTL_4, 0, \"model2_to_model1\")\n","\n","#change_filters(modelGunshotTL_4, modelSirenTL_4, 1, \"model1_to_model2\")\n","change_filters(modelGunshotTL_4, modelSirenTL_4, 1, \"model2_to_model1\")\n","\n","change_filters(modelGunshotTL_4, modelSirenTL_4, 2, \"model1_to_model2\")\n","#change_filters(modelGunshotTL_4, modelSirenTL_4, 2, \"model2_to_model1\")\n","\n","#change_filters(modelGunshotTL_4, modelSirenTL_4, 3, \"model1_to_model2\")\n","change_filters(modelGunshotTL_4, modelSirenTL_4, 3, \"model2_to_model1\")\n","\n","\n","\n","# Evaluating the model on the training and testing set\n","print(\"GUNSHOT: \")\n","score = modelGunshotTL_4.evaluate(x_trainGunshot, y_trainGunshot, verbose=0)\n","print(\"Training Accuracy: \", score[1])\n","\n","score = modelGunshotTL_4.evaluate(x_testGunshot, y_testGunshot, verbose=0)\n","print(\"Testing Accuracy: \", score[1])\n","\n","y_true = np.argmax(y_testGunshot,axis=1)\n","y_pred = np.argmax(modelGunshotTL_4.predict(x_testGunshot),axis=1)\n","print('\\nConfusion Matrix :\\n\\n')\n","print(confusion_matrix(y_true,y_pred))\n","print('\\n\\nClassification Report : \\n\\n',classification_report(y_true,y_pred))\n","\n","\n","print(\"SIREN: \")\n","score = modelSirenTL_4.evaluate(x_trainSiren, y_trainSiren, verbose=0)\n","print(\"Training Accuracy: \", score[1])\n","\n","score = modelSirenTL_4.evaluate(x_testSiren, y_testSiren, verbose=0)\n","print(\"Testing Accuracy: \", score[1])\n","\n","y_true = np.argmax(y_testSiren,axis=1)\n","y_pred = np.argmax(modelSirenTL_4.predict(x_testSiren),axis=1)\n","print('\\nConfusion Matrix :\\n\\n')\n","print(confusion_matrix(y_true,y_pred))\n","print('\\n\\nClassification Report : \\n\\n',classification_report(y_true,y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SFC95XFJ5upj","executionInfo":{"status":"ok","timestamp":1696394837426,"user_tz":300,"elapsed":11259,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}},"outputId":"81e11558-2916-45d8-93f8-8d2afe774d0b"},"execution_count":139,"outputs":[{"output_type":"stream","name":"stdout","text":["GUNSHOT: \n","Training Accuracy:  0.8277592062950134\n","Testing Accuracy:  0.8266666531562805\n","5/5 [==============================] - 1s 61ms/step\n","\n","Confusion Matrix :\n","\n","\n","[[68 12]\n"," [14 56]]\n","\n","\n","Classification Report : \n","\n","               precision    recall  f1-score   support\n","\n","           0       0.83      0.85      0.84        80\n","           1       0.82      0.80      0.81        70\n","\n","    accuracy                           0.83       150\n","   macro avg       0.83      0.82      0.83       150\n","weighted avg       0.83      0.83      0.83       150\n","\n","SIREN: \n","Training Accuracy:  0.9784656763076782\n","Testing Accuracy:  0.9596773982048035\n","12/12 [==============================] - 1s 33ms/step\n","\n","Confusion Matrix :\n","\n","\n","[[188   4]\n"," [ 11 169]]\n","\n","\n","Classification Report : \n","\n","               precision    recall  f1-score   support\n","\n","           0       0.94      0.98      0.96       192\n","           1       0.98      0.94      0.96       180\n","\n","    accuracy                           0.96       372\n","   macro avg       0.96      0.96      0.96       372\n","weighted avg       0.96      0.96      0.96       372\n","\n"]}]},{"cell_type":"markdown","source":["#SCREAM VS SIREN ARQ 4"],"metadata":{"id":"c7Z4UKvg6Boi"}},{"cell_type":"code","source":["\n","modelGunshotTLSave_path_4 = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/Models/Fusion/TL_Complete/gunshot/saved_gunshot_TL_4'\n","modelGunshotTL_4 = tf.keras.models.load_model(modelGunshotTLSave_path_4)\n","\n","modelScreamTLSave_path_4 = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/Models/Fusion/TL_Complete/scream/saved_scream_TL_4'\n","modelScreamTL_4 = tf.keras.models.load_model(modelScreamTLSave_path_4)\n","\n","modelSirenTLSave_path_4 = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/Models/Fusion/TL_Complete/siren/saved_siren_TL_4'\n","modelSirenTL_4 = tf.keras.models.load_model(modelSirenTLSave_path_4)\n","\n","\n","\n","#SCREAM VS SIREN\n","#layer_index = 3  # Specify the index of the layer (e.g., 0 for the first convolutional layer)\n","\n","#change_filters(modelScreamTL_4, modelSirenTL_4, 0, \"model1_to_model2\")\n","change_filters(modelScreamTL_4, modelSirenTL_4, 0, \"model2_to_model1\")\n","\n","#change_filters(modelScreamTL_4, modelSirenTL_4, 1, \"model1_to_model2\")\n","change_filters(modelScreamTL_4, modelSirenTL_4, 1, \"model2_to_model1\")\n","\n","#change_filters(modelScreamTL_4, modelSirenTL_4, 2, \"model1_to_model2\")\n","change_filters(modelScreamTL_4, modelSirenTL_4, 2, \"model2_to_model1\")\n","\n","change_filters(modelScreamTL_4, modelSirenTL_4, 3, \"model1_to_model2\")\n","#change_filters(modelScreamTL_4, modelSirenTL_4, 3, \"model2_to_model1\")\n","\n","\n","\n","# Evaluating the model on the training and testing set\n","print(\"SCREAM: \")\n","score = modelScreamTL_4.evaluate(x_trainScream, y_trainScream, verbose=0)\n","print(\"Training Accuracy: \", score[1])\n","\n","score = modelScreamTL_4.evaluate(x_testScream, y_testScream, verbose=0)\n","print(\"Testing Accuracy: \", score[1])\n","\n","y_true = np.argmax(y_testScream,axis=1)\n","y_pred = np.argmax(modelScreamTL_4.predict(x_testScream),axis=1)\n","print('\\nConfusion Matrix :\\n\\n')\n","print(confusion_matrix(y_true,y_pred))\n","print('\\n\\nClassification Report : \\n\\n',classification_report(y_true,y_pred))\n","\n","\n","print(\"SIREN: \")\n","score = modelSirenTL_4.evaluate(x_trainSiren, y_trainSiren, verbose=0)\n","print(\"Training Accuracy: \", score[1])\n","\n","score = modelSirenTL_4.evaluate(x_testSiren, y_testSiren, verbose=0)\n","print(\"Testing Accuracy: \", score[1])\n","\n","y_true = np.argmax(y_testSiren,axis=1)\n","y_pred = np.argmax(modelSirenTL_4.predict(x_testSiren),axis=1)\n","print('\\nConfusion Matrix :\\n\\n')\n","print(confusion_matrix(y_true,y_pred))\n","print('\\n\\nClassification Report : \\n\\n',classification_report(y_true,y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1nGd5XbP6BK9","executionInfo":{"status":"ok","timestamp":1696395470423,"user_tz":300,"elapsed":17681,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}},"outputId":"7a5f8df4-bf53-4d3d-fc60-48ac0a773f23"},"execution_count":148,"outputs":[{"output_type":"stream","name":"stdout","text":["SCREAM: \n","Training Accuracy:  0.9872000217437744\n","Testing Accuracy:  0.9808917045593262\n","5/5 [==============================] - 1s 70ms/step\n","\n","Confusion Matrix :\n","\n","\n","[[83  1]\n"," [ 2 71]]\n","\n","\n","Classification Report : \n","\n","               precision    recall  f1-score   support\n","\n","           0       0.98      0.99      0.98        84\n","           1       0.99      0.97      0.98        73\n","\n","    accuracy                           0.98       157\n","   macro avg       0.98      0.98      0.98       157\n","weighted avg       0.98      0.98      0.98       157\n","\n","SIREN: \n","Training Accuracy:  0.824360728263855\n","Testing Accuracy:  0.8198924660682678\n","12/12 [==============================] - 0s 22ms/step\n","\n","Confusion Matrix :\n","\n","\n","[[129  63]\n"," [  4 176]]\n","\n","\n","Classification Report : \n","\n","               precision    recall  f1-score   support\n","\n","           0       0.97      0.67      0.79       192\n","           1       0.74      0.98      0.84       180\n","\n","    accuracy                           0.82       372\n","   macro avg       0.85      0.82      0.82       372\n","weighted avg       0.86      0.82      0.82       372\n","\n"]}]},{"cell_type":"markdown","source":["#Creando Nuevo Modelo FUSIONADO"],"metadata":{"id":"D9DbWwicmcPA"}},{"cell_type":"code","source":["# Constructing FUSED model:\n","def getModel(num_rows, num_columns, num_channels, num_labels, k_size):\n","    model = Sequential()\n","    model.add(Conv2D(filters=16, kernel_size=k_size, input_shape=(num_rows, num_columns, num_channels), activation='relu', padding='same'))\n","    model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n","    model.add(Dropout(0.2))\n","\n","    model.add(Conv2D(filters=32, kernel_size=k_size, activation='relu', padding='same'))\n","    model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n","    model.add(Dropout(0.2))\n","\n","    model.add(Conv2D(filters=64, kernel_size=k_size, activation='relu', padding='same'))\n","    model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n","    model.add(Dropout(0.2))\n","\n","    model.add(Conv2D(filters=128, kernel_size=k_size, activation='relu', padding='same'))\n","    model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n","    model.add(Dropout(0.2))\n","    model.add(GlobalAveragePooling2D())\n","    model.add(Flatten())\n","    model.add(Dense(num_labels, activation='softmax'))\n","    return model"],"metadata":{"id":"7QDs5tMJmc_u","executionInfo":{"status":"ok","timestamp":1696396750907,"user_tz":300,"elapsed":391,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}}},"execution_count":195,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, GlobalAveragePooling2D, Flatten, Dense\n","from tensorflow.keras.models import Model\n","\n","def getModelFunctional(num_rows, num_columns, num_channels, num_labels, k_size):\n","    inputs = Input(shape=(num_rows, num_columns, num_channels))\n","    x = Conv2D(filters=16, kernel_size=k_size, activation='relu', padding='same')(inputs)\n","    x = MaxPooling2D(pool_size=(2,2), padding='same')(x)\n","    x = Dropout(0.2)(x)\n","\n","    x = Conv2D(filters=32, kernel_size=k_size, activation='relu', padding='same')(x)\n","    x = MaxPooling2D(pool_size=(2,2), padding='same')(x)\n","    x = Dropout(0.2)(x)\n","\n","    x = Conv2D(filters=64, kernel_size=k_size, activation='relu', padding='same')(x)\n","    x = MaxPooling2D(pool_size=(2,2), padding='same')(x)\n","    x = Dropout(0.2)(x)\n","\n","    x = Conv2D(filters=128, kernel_size=k_size, activation='relu', padding='same')(x)\n","    x = MaxPooling2D(pool_size=(2,2), padding='same')(x)\n","    x = Dropout(0.2)(x)\n","\n","    x = GlobalAveragePooling2D()(x)\n","    x = Flatten()(x)\n","    outputs = Dense(num_labels, activation='softmax')(x)\n","\n","    model = Model(inputs=inputs, outputs=outputs)\n","    return model"],"metadata":{"id":"CAJp3-zcoS0S","executionInfo":{"status":"ok","timestamp":1696396753207,"user_tz":300,"elapsed":3,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}}},"execution_count":196,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, GlobalAveragePooling2D, Flatten, Dense\n","from tensorflow.keras.models import Model\n","\n","def getModelFunctionalModified(num_rows, num_columns, num_channels, num_labels, k_size):\n","    inputs = Input(shape=(num_rows, num_columns, num_channels))\n","    x = Conv2D(filters=16, kernel_size=k_size, activation='relu', padding='same')(inputs)\n","    x = MaxPooling2D(pool_size=(2,2), padding='same')(x)\n","    x = Dropout(0.2)(x)\n","\n","    x = Conv2D(filters=32, kernel_size=k_size, activation='relu', padding='same')(x)\n","    x = MaxPooling2D(pool_size=(2,2), padding='same')(x)\n","    x = Dropout(0.2)(x)\n","\n","    x = Conv2D(filters=64, kernel_size=k_size, activation='relu', padding='same')(x)\n","    x = MaxPooling2D(pool_size=(2,2), padding='same')(x)\n","    x = Dropout(0.2)(x)\n","\n","    x = Conv2D(filters=128, kernel_size=k_size, activation='relu', padding='same')(x)\n","    x = MaxPooling2D(pool_size=(2,2), padding='same')(x)\n","    x = Dropout(0.2)(x)\n","\n","    x = GlobalAveragePooling2D()(x)\n","    x = Flatten()(x)\n","\n","    output1 = Dense(num_labels, activation='softmax', name='output1')(x)\n","    output2 = Dense(num_labels, activation='softmax', name='output2')(x)\n","\n","    model = Model(inputs=inputs, outputs=[output1, output2])\n","    return model"],"metadata":{"id":"wSgH1CO8pjZ7","executionInfo":{"status":"ok","timestamp":1696396754106,"user_tz":300,"elapsed":3,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}}},"execution_count":197,"outputs":[]},{"cell_type":"code","source":["def get_layer_with_params(model, layer_index):\n","    layers_with_params = [layer for layer in model.layers if len(layer.trainable_weights) > 0]\n","\n","    if layer_index < 0 or layer_index >= len(layers_with_params):\n","        raise ValueError(\"Invalid layer index.\")\n","\n","    return layers_with_params[layer_index]\n","\n","def transfer_layer_weights(layer_from, layer_to):\n","    if len(layer_from.trainable_weights) == 0 or len(layer_to.trainable_weights) == 0 or ((len(layer_from.trainable_weights)) != (len(layer_to.trainable_weights))):\n","        return False  # Ambas capas deben tener parámetros entrenables\n","\n","    weights_from, biases_from = layer_from.get_weights()\n","    layer_to.set_weights([weights_from, biases_from])\n","    return True\n","\n","def transfer_parameters_between_models(model_from, layer_index_from, model_to, layer_index_to):\n","    layer_from = get_layer_with_params(model_from, layer_index_from)\n","    layer_to = get_layer_with_params(model_to, layer_index_to)\n","\n","    if transfer_layer_weights(layer_from, layer_to):\n","        print(f\"Transferred weights from layer '{layer_from.name}' to layer '{layer_to.name}'.\")\n","    else:\n","        print(f\"Layers '{layer_from.name}' and '{layer_to.name}' don't have trainable parameters.\")"],"metadata":{"id":"QkjmSK55yAj4","executionInfo":{"status":"ok","timestamp":1696396756051,"user_tz":300,"elapsed":3,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}}},"execution_count":198,"outputs":[]},{"cell_type":"markdown","source":["#EVALUAR FUSIÓN CON CRITERIO BASADO EN MÁXIMO VALOR DE PARÁMETRO\n"],"metadata":{"id":"i5LIPN6g4eXp"}},{"cell_type":"code","source":["def transfer_max_abs_conv_weights(model_from_1, model_from_2, model_to):\n","    layer_names = [layer.name for layer in model_from_1.layers if 'conv' in layer.name]\n","\n","    for layer_index in range(len(layer_names)):\n","      #print(layer_index)\n","      layer_from_1 = get_layer_with_params(model_from_1, layer_index)\n","      layer_from_2 = get_layer_with_params(model_from_2, layer_index)\n","      layer_to = get_layer_with_params(model_to, layer_index)\n","\n","\n","      weights_from_1, biases_from_1 = layer_from_1.get_weights()\n","      weights_from_2, biases_from_2 = layer_from_2.get_weights()\n","\n","      #print(weights_from_1.shape)\n","      #print(weights_from_2.shape)\n","\n","      max_abs_weights = np.array([np.maximum(np.abs(w1), np.abs(w2)) for w1, w2 in zip(weights_from_1, weights_from_2)])\n","      max_abs_biases = np.array([np.maximum(np.abs(b1), np.abs(b2)) for b1, b2 in zip(biases_from_1, biases_from_2)])\n","\n","      #print(max_abs_weights.shape)\n","\n","      layer_to.set_weights([max_abs_weights, max_abs_biases])\n","\n","    return model_to"],"metadata":{"id":"UYNLZ8QH4tYA","executionInfo":{"status":"ok","timestamp":1696396757225,"user_tz":300,"elapsed":3,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}}},"execution_count":199,"outputs":[]},{"cell_type":"markdown","source":["#EVALUAR FUSIÓN CON CRITERIO BASADO EN MÍNIMO VALOR DE PARÁMETRO"],"metadata":{"id":"ksTdDvgyGl57"}},{"cell_type":"code","source":["def transfer_min_abs_conv_weights(model_from_1, model_from_2, model_to):\n","    layer_names = [layer.name for layer in model_from_1.layers if 'conv' in layer.name]\n","\n","    for layer_index in range(len(layer_names)):\n","        #print(layer_index)\n","        layer_from_1 = get_layer_with_params(model_from_1, layer_index)\n","        layer_from_2 = get_layer_with_params(model_from_2, layer_index)\n","        layer_to = get_layer_with_params(model_to, layer_index)\n","\n","        weights_from_1, biases_from_1 = layer_from_1.get_weights()\n","        weights_from_2, biases_from_2 = layer_from_2.get_weights()\n","\n","        #print(weights_from_1.shape)\n","        #print(weights_from_2.shape)\n","\n","        min_abs_weights = np.array([np.minimum(np.abs(w1), np.abs(w2)) for w1, w2 in zip(weights_from_1, weights_from_2)])\n","        min_abs_biases = np.array([np.minimum(np.abs(b1), np.abs(b2)) for b1, b2 in zip(biases_from_1, biases_from_2)])\n","\n","        #print(min_abs_weights.shape)\n","\n","        layer_to.set_weights([min_abs_weights, min_abs_biases])\n","\n","    return model_to"],"metadata":{"id":"ZJpIKN0zGlNZ","executionInfo":{"status":"ok","timestamp":1696396759828,"user_tz":300,"elapsed":4,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}}},"execution_count":200,"outputs":[]},{"cell_type":"markdown","source":["#EVALUAR FUSIÓN CON CRITERIO BASADO EN VALOR PROMEDIO DE PARÁMETROS"],"metadata":{"id":"GGcpc3ucH5RN"}},{"cell_type":"code","source":["def transfer_avg_conv_weights(model_from_1, model_from_2, model_to):\n","    layer_names = [layer.name for layer in model_from_1.layers if 'conv' in layer.name]\n","\n","    for layer_index in range(len(layer_names)):\n","        #print(layer_index)\n","        layer_from_1 = get_layer_with_params(model_from_1, layer_index)\n","        layer_from_2 = get_layer_with_params(model_from_2, layer_index)\n","        layer_to = get_layer_with_params(model_to, layer_index)\n","\n","        weights_from_1, biases_from_1 = layer_from_1.get_weights()\n","        weights_from_2, biases_from_2 = layer_from_2.get_weights()\n","\n","        #print(weights_from_1.shape)\n","        #print(weights_from_2.shape)\n","\n","        avg_weights = np.array([(w1 + w2) / 2 for w1, w2 in zip(weights_from_1, weights_from_2)])\n","        avg_biases = np.array([(b1 + b2) / 2 for b1, b2 in zip(biases_from_1, biases_from_2)])\n","\n","        #print(avg_weights.shape)\n","\n","        layer_to.set_weights([avg_weights, avg_biases])\n","\n","    return model_to"],"metadata":{"id":"WmGJeTZRH5ex","executionInfo":{"status":"ok","timestamp":1696396761147,"user_tz":300,"elapsed":6,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}}},"execution_count":201,"outputs":[]},{"cell_type":"markdown","source":["#GUN VS SCREAM ARQ 1 MAX/MIN/AVG VALUE IN LAYER"],"metadata":{"id":"Tigj5tBIh9Ro"}},{"cell_type":"code","source":["#1\t3\t4096\t4096\t4096\t0\t2\n","#2\t12\t4096\t4096\t4096\t0\t3\n","#3\t27\t4096\t4096\t4096\t0\t3\n","#4\t45\t4096\t4096\t4096\t0\t5\n","\n","samplerate = 22050\n","longitudMaxAudio = 4\n","Nmfcc = 3\n","Nfft = 4096\n","NwinL = 4096\n","iterableNhopL = 1.0\n","NhopL =  4096       #int(iterableNhopL*NwinL)\n","k_size = 2\n","num_rows = Nmfcc\n","num_columns = int(samplerate*longitudMaxAudio/NhopL) + int(samplerate*longitudMaxAudio/NhopL*0.05)  #Calculo longitud de salida de mfcc con 5% de tolerancia para longitud de audios\n","num_channels = 1\n","\n","num_labels = 2 #4\n","\n","#model = getModel(num_rows, num_columns, num_channels, num_labels, k_size)\n","#model = getModelFunctional(num_rows, num_columns, num_channels, num_labels, k_size)\n","Fusedmodel = getModelFunctionalModified(num_rows, num_columns, num_channels, num_labels, k_size)\n","\n","# Compila el modelo (es necesario antes de la evaluación)\n","Fusedmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","#TRAINED MODELS TO FUSE\n","modelGunshotTLSave_path_1 = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/Models/Fusion/TL_Complete/gunshot/saved_gunshot_TL_1'\n","modelGunshotTL_1 = tf.keras.models.load_model(modelGunshotTLSave_path_1)\n","\n","modelScreamTLSave_path_1 = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/Models/Fusion/TL_Complete/scream/saved_scream_TL_1'\n","modelScreamTL_1 = tf.keras.models.load_model(modelScreamTLSave_path_1)\n","\n","#layer_index = 3  # Specify the index of the layer (e.g., 0 for the first convolutional layer)\n","#change_filters(modelGunshotTL, modelScreamTL, 0, \"model1_to_model2\")\n","#change_filters(modelGunshotTL, modelScreamTL, 1, \"model1_to_model2\")\n","#change_filters(modelGunshotTL, modelScreamTL, 2, \"model2_to_model1\")\n","#change_filters(modelGunshotTL, modelScreamTL, layer_index, \"model1_to_model2\")\n","\n","# Iterar sobre las capas y verificar la cantidad de parámetros entrenables\n","for layer in Fusedmodel.layers:\n","    trainable_params_count = layer.count_params()\n","    if trainable_params_count > 0:\n","        print(\"Layer Name:\", layer.name)\n","        print(\"Trainable Parameters Count:\", trainable_params_count)\n","print(\"------------------------\")\n","\n","# Iterar sobre las capas y verificar la cantidad de parámetros entrenables\n","for layer in modelGunshotTL_1.layers:\n","    trainable_params_count = layer.count_params()\n","    if trainable_params_count > 0:\n","        print(\"Layer Name:\", layer.name)\n","        print(\"Trainable Parameters Count:\", trainable_params_count)\n","print(\"------------------------\")\n","\n","\n","#transfer_parameters_between_models(modelGunshotTL, 0, Fusedmodel, 0)\n","#transfer_parameters_between_models(modelGunshotTL, 1, Fusedmodel, 1)\n","#transfer_parameters_between_models(modelScreamTL, 2, Fusedmodel, 2)\n","#transfer_parameters_between_models(modelGunshotTL, 3, Fusedmodel, 3)\n","\n","\n","\n","#transfer_max_abs_conv_weights(modelGunshotTL_1, modelScreamTL_1, Fusedmodel)\n","#transfer_min_abs_conv_weights(modelGunshotTL_1, modelScreamTL_1, Fusedmodel)\n","transfer_avg_conv_weights(modelGunshotTL_1, modelScreamTL_1, Fusedmodel)\n","\n","transfer_parameters_between_models(modelGunshotTL_1, 4, Fusedmodel, 4)\n","transfer_parameters_between_models(modelScreamTL_1, 4, Fusedmodel, 5)\n","\n","\n","#Fusedmodel.summary()\n","\n","\n","# Evaluating the FUSED model on the training and testing sets\n","\n","scoreGunshot = Fusedmodel.evaluate(x_trainGunshot, [y_trainGunshot, np.resize(y_trainScream, (y_trainGunshot.shape[0], *y_trainScream.shape[1:]))], verbose=0)\n","print(\"OUT Gunshot: \", scoreGunshot)\n","print(\"Training Accuracy for Gunshot: \", scoreGunshot[3])\n","\n","scoreGunshot = Fusedmodel.evaluate(x_testGunshot, [y_testGunshot, np.resize(y_testScream, (y_testGunshot.shape[0], *y_testScream.shape[1:]))], verbose=0)\n","print(\"OUT Gunshot: \", scoreGunshot)\n","print(\"Testing Accuracy for Gunshot: \", scoreGunshot[3])\n","\n","\n","\n","scoreScream= Fusedmodel.evaluate(x_trainScream, [np.resize(y_trainGunshot, (y_trainScream.shape[0], *y_trainGunshot.shape[1:])), y_trainScream], verbose=0)\n","print(\"OUT Scream: \", scoreScream)\n","print(\"Training Accuracy for Scream: \", scoreScream[4])\n","\n","scoreScream= Fusedmodel.evaluate(x_testScream, [np.resize(y_testGunshot, (y_testScream.shape[0], *y_testGunshot.shape[1:])), y_testScream], verbose=0)\n","print(\"OUT Scream: \", scoreScream)\n","print(\"Testing Accuracy for Scream: \", scoreScream[4])\n","\n","\n","y_true = np.argmax(y_testGunshot,axis=1)\n","y_pred = np.argmax(Fusedmodel.predict(x_testGunshot)[0],axis=1)\n","print('\\nConfusion Matrix GUNSHOT:\\n\\n')\n","print('SHAPEEE : ',y_true.shape)\n","print('SHAPEEE : ',y_pred.shape)\n","print(confusion_matrix(y_true,y_pred))\n","print('\\n\\nClassification Report : \\n\\n',classification_report(y_true,y_pred))\n","\n","y_true = np.argmax(y_testScream,axis=1)\n","y_pred = np.argmax(Fusedmodel.predict(x_testScream)[1],axis=1)\n","print('\\nConfusion Matrix SCREAM:\\n\\n')\n","print('SHAPEEE : ',y_true.shape)\n","print('SHAPEEE : ',y_pred.shape)\n","print(confusion_matrix(y_true,y_pred))\n","print('\\n\\nClassification Report : \\n\\n',classification_report(y_true,y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CIL0oVgjm_we","executionInfo":{"status":"ok","timestamp":1696396872420,"user_tz":300,"elapsed":5821,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}},"outputId":"837ea0ae-8dca-4a7f-a9bd-6c568c48215d"},"execution_count":204,"outputs":[{"output_type":"stream","name":"stdout","text":["Layer Name: conv2d_88\n","Trainable Parameters Count: 80\n","Layer Name: conv2d_89\n","Trainable Parameters Count: 2080\n","Layer Name: conv2d_90\n","Trainable Parameters Count: 8256\n","Layer Name: conv2d_91\n","Trainable Parameters Count: 32896\n","Layer Name: output1\n","Trainable Parameters Count: 258\n","Layer Name: output2\n","Trainable Parameters Count: 258\n","------------------------\n","Layer Name: conv2d\n","Trainable Parameters Count: 80\n","Layer Name: conv2d_1\n","Trainable Parameters Count: 2080\n","Layer Name: conv2d_2\n","Trainable Parameters Count: 8256\n","Layer Name: conv2d_3\n","Trainable Parameters Count: 32896\n","Layer Name: dense\n","Trainable Parameters Count: 258\n","------------------------\n","Transferred weights from layer 'dense' to layer 'output1'.\n","Transferred weights from layer 'dense' to layer 'output2'.\n","OUT Gunshot:  [1.90944242477417, 0.2235431969165802, 1.685899257659912, 0.9180601835250854, 0.5100334286689758]\n","Training Accuracy for Gunshot:  0.9180601835250854\n","OUT Gunshot:  [1.7039003372192383, 0.23468945920467377, 1.4692109823226929, 0.9133333563804626, 0.5400000214576721]\n","Testing Accuracy for Gunshot:  0.9133333563804626\n","OUT Scream:  [1.5052814483642578, 1.4188683032989502, 0.08641317486763, 0.5008000135421753, 0.9855999946594238]\n","Training Accuracy for Scream:  0.9855999946594238\n","OUT Scream:  [1.4478687047958374, 1.3500779867172241, 0.09779065102338791, 0.5159235596656799, 0.9745222926139832]\n","Testing Accuracy for Scream:  0.9745222926139832\n","5/5 [==============================] - 0s 5ms/step\n","\n","Confusion Matrix GUNSHOT:\n","\n","\n","SHAPEEE :  (150,)\n","SHAPEEE :  (150,)\n","[[72  8]\n"," [ 5 65]]\n","\n","\n","Classification Report : \n","\n","               precision    recall  f1-score   support\n","\n","           0       0.94      0.90      0.92        80\n","           1       0.89      0.93      0.91        70\n","\n","    accuracy                           0.91       150\n","   macro avg       0.91      0.91      0.91       150\n","weighted avg       0.91      0.91      0.91       150\n","\n","5/5 [==============================] - 0s 6ms/step\n","\n","Confusion Matrix SCREAM:\n","\n","\n","SHAPEEE :  (157,)\n","SHAPEEE :  (157,)\n","[[83  1]\n"," [ 3 70]]\n","\n","\n","Classification Report : \n","\n","               precision    recall  f1-score   support\n","\n","           0       0.97      0.99      0.98        84\n","           1       0.99      0.96      0.97        73\n","\n","    accuracy                           0.97       157\n","   macro avg       0.98      0.97      0.97       157\n","weighted avg       0.97      0.97      0.97       157\n","\n"]}]},{"cell_type":"markdown","source":["#GUN VS SIREN ARQ 1 MAX/MIN/AVG VALUE IN LAYER"],"metadata":{"id":"l2hMhkQWiEne"}},{"cell_type":"code","source":["#1\t3\t4096\t4096\t4096\t0\t2\n","#2\t12\t4096\t4096\t4096\t0\t3\n","#3\t27\t4096\t4096\t4096\t0\t3\n","#4\t45\t4096\t4096\t4096\t0\t5\n","\n","samplerate = 22050\n","longitudMaxAudio = 4\n","Nmfcc = 3\n","Nfft = 4096\n","NwinL = 4096\n","iterableNhopL = 1.0\n","NhopL =  4096       #int(iterableNhopL*NwinL)\n","k_size = 2\n","num_rows = Nmfcc\n","num_columns = int(samplerate*longitudMaxAudio/NhopL) + int(samplerate*longitudMaxAudio/NhopL*0.05)  #Calculo longitud de salida de mfcc con 5% de tolerancia para longitud de audios\n","num_channels = 1\n","\n","num_labels = 2 #4\n","\n","#model = getModel(num_rows, num_columns, num_channels, num_labels, k_size)\n","#model = getModelFunctional(num_rows, num_columns, num_channels, num_labels, k_size)\n","Fusedmodel = getModelFunctionalModified(num_rows, num_columns, num_channels, num_labels, k_size)\n","\n","# Compila el modelo (es necesario antes de la evaluación)\n","Fusedmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","#TRAINED MODELS TO FUSE\n","modelGunshotTLSave_path_1 = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/Models/Fusion/TL_Complete/gunshot/saved_gunshot_TL_1'\n","modelGunshotTL_1 = tf.keras.models.load_model(modelGunshotTLSave_path_1)\n","\n","modelSirenTLSave_path_1 = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/Models/Fusion/TL_Complete/siren/saved_siren_TL_1'\n","modelSirenTL_1 = tf.keras.models.load_model(modelSirenTLSave_path_1)\n","\n","#layer_index = 3  # Specify the index of the layer (e.g., 0 for the first convolutional layer)\n","#change_filters(modelGunshotTL, modelScreamTL, 0, \"model1_to_model2\")\n","#change_filters(modelGunshotTL, modelScreamTL, 1, \"model1_to_model2\")\n","#change_filters(modelGunshotTL, modelScreamTL, 2, \"model2_to_model1\")\n","#change_filters(modelGunshotTL, modelScreamTL, layer_index, \"model1_to_model2\")\n","\n","# Iterar sobre las capas y verificar la cantidad de parámetros entrenables\n","for layer in Fusedmodel.layers:\n","    trainable_params_count = layer.count_params()\n","    if trainable_params_count > 0:\n","        print(\"Layer Name:\", layer.name)\n","        print(\"Trainable Parameters Count:\", trainable_params_count)\n","print(\"------------------------\")\n","\n","# Iterar sobre las capas y verificar la cantidad de parámetros entrenables\n","for layer in modelGunshotTL_1.layers:\n","    trainable_params_count = layer.count_params()\n","    if trainable_params_count > 0:\n","        print(\"Layer Name:\", layer.name)\n","        print(\"Trainable Parameters Count:\", trainable_params_count)\n","print(\"------------------------\")\n","\n","\n","#transfer_parameters_between_models(modelGunshotTL, 0, Fusedmodel, 0)\n","#transfer_parameters_between_models(modelGunshotTL, 1, Fusedmodel, 1)\n","#transfer_parameters_between_models(modelScreamTL, 2, Fusedmodel, 2)\n","#transfer_parameters_between_models(modelGunshotTL, 3, Fusedmodel, 3)\n","\n","\n","\n","#transfer_max_abs_conv_weights(modelGunshotTL_1, modelSirenTL_1, Fusedmodel)\n","#transfer_min_abs_conv_weights(modelGunshotTL_1, modelSirenTL_1, Fusedmodel)\n","transfer_avg_conv_weights(modelGunshotTL_1, modelSirenTL_1, Fusedmodel)\n","\n","transfer_parameters_between_models(modelGunshotTL_1, 4, Fusedmodel, 4)\n","transfer_parameters_between_models(modelSirenTL_1, 4, Fusedmodel, 5)\n","\n","\n","#Fusedmodel.summary()\n","\n","\n","# Evaluating the FUSED model on the training and testing sets\n","\n","scoreGunshot = Fusedmodel.evaluate(x_trainGunshot, [y_trainGunshot, np.resize(y_trainSiren, (y_trainGunshot.shape[0], *y_trainSiren.shape[1:]))], verbose=0)\n","print(\"OUT Gunshot: \", scoreGunshot)\n","print(\"Training Accuracy for Gunshot: \", scoreGunshot[3])\n","\n","scoreGunshot = Fusedmodel.evaluate(x_testGunshot, [y_testGunshot, np.resize(y_testSiren, (y_testGunshot.shape[0], *y_testSiren.shape[1:]))], verbose=0)\n","print(\"OUT Gunshot: \", scoreGunshot)\n","print(\"Testing Accuracy for Gunshot: \", scoreGunshot[3])\n","\n","\n","\n","scoreSiren= Fusedmodel.evaluate(x_trainSiren, [np.resize(y_trainGunshot, (y_trainSiren.shape[0], *y_trainGunshot.shape[1:])), y_trainSiren], verbose=0)\n","print(\"OUT Siren: \", scoreSiren)\n","print(\"Training Accuracy for Siren: \", scoreSiren[4])\n","\n","scoreSiren= Fusedmodel.evaluate(x_testSiren, [np.resize(y_testGunshot, (y_testSiren.shape[0], *y_testGunshot.shape[1:])), y_testSiren], verbose=0)\n","print(\"OUT Siren: \", scoreSiren)\n","print(\"Testing Accuracy for Siren: \", scoreSiren[4])\n","\n","\n","y_true = np.argmax(y_testGunshot,axis=1)\n","y_pred = np.argmax(Fusedmodel.predict(x_testGunshot)[0],axis=1)\n","print('\\nConfusion Matrix GUNSHOT:\\n\\n')\n","print('SHAPEEE : ',y_true.shape)\n","print('SHAPEEE : ',y_pred.shape)\n","print(confusion_matrix(y_true,y_pred))\n","print('\\n\\nClassification Report : \\n\\n',classification_report(y_true,y_pred))\n","\n","y_true = np.argmax(y_testSiren,axis=1)\n","y_pred = np.argmax(Fusedmodel.predict(x_testSiren)[1],axis=1)\n","print('\\nConfusion Matrix SIREN:\\n\\n')\n","print('SHAPEEE : ',y_true.shape)\n","print('SHAPEEE : ',y_pred.shape)\n","print(confusion_matrix(y_true,y_pred))\n","print('\\n\\nClassification Report : \\n\\n',classification_report(y_true,y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oKhZvrcKiKby","executionInfo":{"status":"ok","timestamp":1696396986958,"user_tz":300,"elapsed":4918,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}},"outputId":"9858778c-dd1f-403e-c092-55a58c01f8af"},"execution_count":207,"outputs":[{"output_type":"stream","name":"stdout","text":["Layer Name: conv2d_100\n","Trainable Parameters Count: 80\n","Layer Name: conv2d_101\n","Trainable Parameters Count: 2080\n","Layer Name: conv2d_102\n","Trainable Parameters Count: 8256\n","Layer Name: conv2d_103\n","Trainable Parameters Count: 32896\n","Layer Name: output1\n","Trainable Parameters Count: 258\n","Layer Name: output2\n","Trainable Parameters Count: 258\n","------------------------\n","Layer Name: conv2d\n","Trainable Parameters Count: 80\n","Layer Name: conv2d_1\n","Trainable Parameters Count: 2080\n","Layer Name: conv2d_2\n","Trainable Parameters Count: 8256\n","Layer Name: conv2d_3\n","Trainable Parameters Count: 32896\n","Layer Name: dense\n","Trainable Parameters Count: 258\n","------------------------\n","Transferred weights from layer 'dense' to layer 'output1'.\n","Transferred weights from layer 'dense' to layer 'output2'.\n","OUT Gunshot:  [1.421236515045166, 0.1844685971736908, 1.2367676496505737, 0.9414715766906738, 0.5117056965827942]\n","Training Accuracy for Gunshot:  0.9414715766906738\n","OUT Gunshot:  [1.331782579421997, 0.19050715863704681, 1.141275405883789, 0.95333331823349, 0.5333333611488342]\n","Testing Accuracy for Gunshot:  0.95333331823349\n","OUT Siren:  [2.0174195766448975, 1.4753113985061646, 0.5421079993247986, 0.5067294836044312, 0.7220726609230042]\n","Training Accuracy for Siren:  0.7220726609230042\n","OUT Siren:  [2.0074167251586914, 1.4529201984405518, 0.5544962882995605, 0.5107526779174805, 0.7096773982048035]\n","Testing Accuracy for Siren:  0.7096773982048035\n","5/5 [==============================] - 0s 5ms/step\n","\n","Confusion Matrix GUNSHOT:\n","\n","\n","SHAPEEE :  (150,)\n","SHAPEEE :  (150,)\n","[[73  7]\n"," [ 0 70]]\n","\n","\n","Classification Report : \n","\n","               precision    recall  f1-score   support\n","\n","           0       1.00      0.91      0.95        80\n","           1       0.91      1.00      0.95        70\n","\n","    accuracy                           0.95       150\n","   macro avg       0.95      0.96      0.95       150\n","weighted avg       0.96      0.95      0.95       150\n","\n","12/12 [==============================] - 0s 6ms/step\n","\n","Confusion Matrix SIREN:\n","\n","\n","SHAPEEE :  (372,)\n","SHAPEEE :  (372,)\n","[[169  23]\n"," [ 85  95]]\n","\n","\n","Classification Report : \n","\n","               precision    recall  f1-score   support\n","\n","           0       0.67      0.88      0.76       192\n","           1       0.81      0.53      0.64       180\n","\n","    accuracy                           0.71       372\n","   macro avg       0.74      0.70      0.70       372\n","weighted avg       0.73      0.71      0.70       372\n","\n"]}]},{"cell_type":"markdown","source":["#SCREAM VS SIREN ARQ 1 MAX/MIN/AVG VALUE IN LAYER"],"metadata":{"id":"GnHmHsd0jQlB"}},{"cell_type":"code","source":["#1\t3\t4096\t4096\t4096\t0\t2\n","#2\t12\t4096\t4096\t4096\t0\t3\n","#3\t27\t4096\t4096\t4096\t0\t3\n","#4\t45\t4096\t4096\t4096\t0\t5\n","\n","samplerate = 22050\n","longitudMaxAudio = 4\n","Nmfcc = 3\n","Nfft = 4096\n","NwinL = 4096\n","iterableNhopL = 1.0\n","NhopL =  4096       #int(iterableNhopL*NwinL)\n","k_size = 2\n","num_rows = Nmfcc\n","num_columns = int(samplerate*longitudMaxAudio/NhopL) + int(samplerate*longitudMaxAudio/NhopL*0.05)  #Calculo longitud de salida de mfcc con 5% de tolerancia para longitud de audios\n","num_channels = 1\n","\n","num_labels = 2 #4\n","\n","#model = getModel(num_rows, num_columns, num_channels, num_labels, k_size)\n","#model = getModelFunctional(num_rows, num_columns, num_channels, num_labels, k_size)\n","Fusedmodel = getModelFunctionalModified(num_rows, num_columns, num_channels, num_labels, k_size)\n","\n","# Compila el modelo (es necesario antes de la evaluación)\n","Fusedmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","#TRAINED MODELS TO FUSE\n","modelScreamTLSave_path_1 = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/Models/Fusion/TL_Complete/scream/saved_scream_TL_1'\n","modelScreamTL_1 = tf.keras.models.load_model(modelScreamTLSave_path_1)\n","\n","modelSirenTLSave_path_1 = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/Models/Fusion/TL_Complete/siren/saved_siren_TL_1'\n","modelSirenTL_1 = tf.keras.models.load_model(modelSirenTLSave_path_1)\n","\n","#layer_index = 3  # Specify the index of the layer (e.g., 0 for the first convolutional layer)\n","#change_filters(modelGunshotTL, modelScreamTL, 0, \"model1_to_model2\")\n","#change_filters(modelGunshotTL, modelScreamTL, 1, \"model1_to_model2\")\n","#change_filters(modelGunshotTL, modelScreamTL, 2, \"model2_to_model1\")\n","#change_filters(modelGunshotTL, modelScreamTL, layer_index, \"model1_to_model2\")\n","\n","# Iterar sobre las capas y verificar la cantidad de parámetros entrenables\n","for layer in Fusedmodel.layers:\n","    trainable_params_count = layer.count_params()\n","    if trainable_params_count > 0:\n","        print(\"Layer Name:\", layer.name)\n","        print(\"Trainable Parameters Count:\", trainable_params_count)\n","print(\"------------------------\")\n","\n","# Iterar sobre las capas y verificar la cantidad de parámetros entrenables\n","for layer in modelScreamTL_1.layers:\n","    trainable_params_count = layer.count_params()\n","    if trainable_params_count > 0:\n","        print(\"Layer Name:\", layer.name)\n","        print(\"Trainable Parameters Count:\", trainable_params_count)\n","print(\"------------------------\")\n","\n","\n","#transfer_parameters_between_models(modelGunshotTL, 0, Fusedmodel, 0)\n","#transfer_parameters_between_models(modelGunshotTL, 1, Fusedmodel, 1)\n","#transfer_parameters_between_models(modelScreamTL, 2, Fusedmodel, 2)\n","#transfer_parameters_between_models(modelGunshotTL, 3, Fusedmodel, 3)\n","\n","\n","\n","#transfer_max_abs_conv_weights(modelScreamTL_1, modelSirenTL_1, Fusedmodel)\n","#transfer_min_abs_conv_weights(modelScreamTL_1, modelSirenTL_1, Fusedmodel)\n","transfer_avg_conv_weights(modelScreamTL_1, modelSirenTL_1, Fusedmodel)\n","\n","transfer_parameters_between_models(modelScreamTL_1, 4, Fusedmodel, 4)\n","transfer_parameters_between_models(modelSirenTL_1, 4, Fusedmodel, 5)\n","\n","\n","#Fusedmodel.summary()\n","\n","\n","# Evaluating the FUSED model on the training and testing sets\n","\n","scoreScream = Fusedmodel.evaluate(x_trainScream, [y_trainScream, np.resize(y_trainSiren, (y_trainScream.shape[0], *y_trainSiren.shape[1:]))], verbose=0)\n","print(\"OUT Scream: \", scoreScream)\n","print(\"Training Accuracy for Scream: \", scoreScream[3])\n","\n","scoreScream = Fusedmodel.evaluate(x_testScream, [y_testScream, np.resize(y_testSiren, (y_testScream.shape[0], *y_testSiren.shape[1:]))], verbose=0)\n","print(\"OUT Scream: \", scoreScream)\n","print(\"Testing Accuracy for Scream: \", scoreScream[3])\n","\n","\n","\n","scoreSiren= Fusedmodel.evaluate(x_trainSiren, [np.resize(y_trainScream, (y_trainSiren.shape[0], *y_trainScream.shape[1:])), y_trainSiren], verbose=0)\n","print(\"OUT Siren: \", scoreSiren)\n","print(\"Training Accuracy for Siren: \", scoreSiren[4])\n","\n","scoreSiren= Fusedmodel.evaluate(x_testSiren, [np.resize(y_testScream, (y_testSiren.shape[0], *y_testScream.shape[1:])), y_testSiren], verbose=0)\n","print(\"OUT Siren: \", scoreSiren)\n","print(\"Testing Accuracy for Siren: \", scoreSiren[4])\n","\n","\n","y_true = np.argmax(y_testScream,axis=1)\n","y_pred = np.argmax(Fusedmodel.predict(x_testScream)[0],axis=1)\n","print('\\nConfusion Matrix SCREAM:\\n\\n')\n","print('SHAPEEE : ',y_true.shape)\n","print('SHAPEEE : ',y_pred.shape)\n","print(confusion_matrix(y_true,y_pred))\n","print('\\n\\nClassification Report : \\n\\n',classification_report(y_true,y_pred))\n","\n","y_true = np.argmax(y_testSiren,axis=1)\n","y_pred = np.argmax(Fusedmodel.predict(x_testSiren)[1],axis=1)\n","print('\\nConfusion Matrix SIREN:\\n\\n')\n","print('SHAPEEE : ',y_true.shape)\n","print('SHAPEEE : ',y_pred.shape)\n","print(confusion_matrix(y_true,y_pred))\n","print('\\n\\nClassification Report : \\n\\n',classification_report(y_true,y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"anZ1NoZXjS1c","executionInfo":{"status":"ok","timestamp":1696397195142,"user_tz":300,"elapsed":6205,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}},"outputId":"67014ce7-d747-4990-fd86-241f910cda67"},"execution_count":210,"outputs":[{"output_type":"stream","name":"stdout","text":["Layer Name: conv2d_112\n","Trainable Parameters Count: 80\n","Layer Name: conv2d_113\n","Trainable Parameters Count: 2080\n","Layer Name: conv2d_114\n","Trainable Parameters Count: 8256\n","Layer Name: conv2d_115\n","Trainable Parameters Count: 32896\n","Layer Name: output1\n","Trainable Parameters Count: 258\n","Layer Name: output2\n","Trainable Parameters Count: 258\n","------------------------\n","Layer Name: conv2d\n","Trainable Parameters Count: 80\n","Layer Name: conv2d_1\n","Trainable Parameters Count: 2080\n","Layer Name: conv2d_2\n","Trainable Parameters Count: 8256\n","Layer Name: conv2d_3\n","Trainable Parameters Count: 32896\n","Layer Name: dense\n","Trainable Parameters Count: 258\n","------------------------\n","Transferred weights from layer 'dense' to layer 'output1'.\n","Transferred weights from layer 'dense' to layer 'output2'.\n","OUT Scream:  [1.0257936716079712, 0.08844684809446335, 0.9373467564582825, 0.984000027179718, 0.5216000080108643]\n","Training Accuracy for Scream:  0.984000027179718\n","OUT Scream:  [0.9769054651260376, 0.10497266054153442, 0.8719328045845032, 0.9745222926139832, 0.522292971611023]\n","Testing Accuracy for Scream:  0.9745222926139832\n","OUT Siren:  [2.4709017276763916, 2.014516592025757, 0.4563848674297333, 0.49327051639556885, 0.7812920808792114]\n","Training Accuracy for Siren:  0.7812920808792114\n","OUT Siren:  [2.2970385551452637, 1.8157120943069458, 0.4813264310359955, 0.5483871102333069, 0.7768816947937012]\n","Testing Accuracy for Siren:  0.7768816947937012\n","5/5 [==============================] - 0s 9ms/step\n","\n","Confusion Matrix SCREAM:\n","\n","\n","SHAPEEE :  (157,)\n","SHAPEEE :  (157,)\n","[[83  1]\n"," [ 3 70]]\n","\n","\n","Classification Report : \n","\n","               precision    recall  f1-score   support\n","\n","           0       0.97      0.99      0.98        84\n","           1       0.99      0.96      0.97        73\n","\n","    accuracy                           0.97       157\n","   macro avg       0.98      0.97      0.97       157\n","weighted avg       0.97      0.97      0.97       157\n","\n","12/12 [==============================] - 0s 7ms/step\n","\n","Confusion Matrix SIREN:\n","\n","\n","SHAPEEE :  (372,)\n","SHAPEEE :  (372,)\n","[[150  42]\n"," [ 41 139]]\n","\n","\n","Classification Report : \n","\n","               precision    recall  f1-score   support\n","\n","           0       0.79      0.78      0.78       192\n","           1       0.77      0.77      0.77       180\n","\n","    accuracy                           0.78       372\n","   macro avg       0.78      0.78      0.78       372\n","weighted avg       0.78      0.78      0.78       372\n","\n"]}]},{"cell_type":"markdown","source":["#GUN VS SCREAM ARQ 4 MAX/MIN/AVG VALUE IN LAYER"],"metadata":{"id":"6-v-vvgRn-pj"}},{"cell_type":"code","source":["#1\t3\t4096\t4096\t4096\t0\t2\n","#2\t12\t4096\t4096\t4096\t0\t3\n","#3\t27\t4096\t4096\t4096\t0\t3\n","#4\t45\t4096\t4096\t4096\t0\t5\n","\n","samplerate = 22050\n","longitudMaxAudio = 4\n","Nmfcc = 45\n","Nfft = 4096\n","NwinL = 4096\n","iterableNhopL = 1.0\n","NhopL =  4096       #int(iterableNhopL*NwinL)\n","k_size = 5\n","num_rows = Nmfcc\n","num_columns = int(samplerate*longitudMaxAudio/NhopL) + int(samplerate*longitudMaxAudio/NhopL*0.05)  #Calculo longitud de salida de mfcc con 5% de tolerancia para longitud de audios\n","num_channels = 1\n","\n","num_labels = 2 #4\n","\n","#model = getModel(num_rows, num_columns, num_channels, num_labels, k_size)\n","#model = getModelFunctional(num_rows, num_columns, num_channels, num_labels, k_size)\n","Fusedmodel = getModelFunctionalModified(num_rows, num_columns, num_channels, num_labels, k_size)\n","\n","# Compila el modelo (es necesario antes de la evaluación)\n","Fusedmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","#TRAINED MODELS TO FUSE\n","modelGunshotTLSave_path_4 = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/Models/Fusion/TL_Complete/gunshot/saved_gunshot_TL_4'\n","modelGunshotTL_4 = tf.keras.models.load_model(modelGunshotTLSave_path_4)\n","\n","modelScreamTLSave_path_4 = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/Models/Fusion/TL_Complete/scream/saved_scream_TL_4'\n","modelScreamTL_4 = tf.keras.models.load_model(modelScreamTLSave_path_4)\n","\n","#layer_index = 3  # Specify the index of the layer (e.g., 0 for the first convolutional layer)\n","#change_filters(modelGunshotTL, modelScreamTL, 0, \"model1_to_model2\")\n","#change_filters(modelGunshotTL, modelScreamTL, 1, \"model1_to_model2\")\n","#change_filters(modelGunshotTL, modelScreamTL, 2, \"model2_to_model1\")\n","#change_filters(modelGunshotTL, modelScreamTL, layer_index, \"model1_to_model2\")\n","\n","# Iterar sobre las capas y verificar la cantidad de parámetros entrenables\n","for layer in Fusedmodel.layers:\n","    trainable_params_count = layer.count_params()\n","    if trainable_params_count > 0:\n","        print(\"Layer Name:\", layer.name)\n","        print(\"Trainable Parameters Count:\", trainable_params_count)\n","print(\"------------------------\")\n","\n","# Iterar sobre las capas y verificar la cantidad de parámetros entrenables\n","for layer in modelGunshotTL_4.layers:\n","    trainable_params_count = layer.count_params()\n","    if trainable_params_count > 0:\n","        print(\"Layer Name:\", layer.name)\n","        print(\"Trainable Parameters Count:\", trainable_params_count)\n","print(\"------------------------\")\n","\n","\n","#transfer_parameters_between_models(modelGunshotTL, 0, Fusedmodel, 0)\n","#transfer_parameters_between_models(modelGunshotTL, 1, Fusedmodel, 1)\n","#transfer_parameters_between_models(modelScreamTL, 2, Fusedmodel, 2)\n","#transfer_parameters_between_models(modelGunshotTL, 3, Fusedmodel, 3)\n","\n","\n","\n","#transfer_max_abs_conv_weights(modelGunshotTL_4, modelScreamTL_4, Fusedmodel)\n","#transfer_min_abs_conv_weights(modelGunshotTL_4, modelScreamTL_4, Fusedmodel)\n","transfer_avg_conv_weights(modelGunshotTL_4, modelScreamTL_4, Fusedmodel)\n","\n","transfer_parameters_between_models(modelGunshotTL_4, 4, Fusedmodel, 4)\n","transfer_parameters_between_models(modelScreamTL_4, 4, Fusedmodel, 5)\n","\n","\n","#Fusedmodel.summary()\n","\n","\n","# Evaluating the FUSED model on the training and testing sets\n","\n","scoreGunshot = Fusedmodel.evaluate(x_trainGunshot, [y_trainGunshot, np.resize(y_trainScream, (y_trainGunshot.shape[0], *y_trainScream.shape[1:]))], verbose=0)\n","print(\"OUT Gunshot: \", scoreGunshot)\n","print(\"Training Accuracy for Gunshot: \", scoreGunshot[3])\n","\n","scoreGunshot = Fusedmodel.evaluate(x_testGunshot, [y_testGunshot, np.resize(y_testScream, (y_testGunshot.shape[0], *y_testScream.shape[1:]))], verbose=0)\n","print(\"OUT Gunshot: \", scoreGunshot)\n","print(\"Testing Accuracy for Gunshot: \", scoreGunshot[3])\n","\n","\n","\n","scoreScream= Fusedmodel.evaluate(x_trainScream, [np.resize(y_trainGunshot, (y_trainScream.shape[0], *y_trainGunshot.shape[1:])), y_trainScream], verbose=0)\n","print(\"OUT Scream: \", scoreScream)\n","print(\"Training Accuracy for Scream: \", scoreScream[4])\n","\n","scoreScream= Fusedmodel.evaluate(x_testScream, [np.resize(y_testGunshot, (y_testScream.shape[0], *y_testGunshot.shape[1:])), y_testScream], verbose=0)\n","print(\"OUT Scream: \", scoreScream)\n","print(\"Testing Accuracy for Scream: \", scoreScream[4])\n","\n","\n","y_true = np.argmax(y_testGunshot,axis=1)\n","y_pred = np.argmax(Fusedmodel.predict(x_testGunshot)[0],axis=1)\n","print('\\nConfusion Matrix GUNSHOT:\\n\\n')\n","print('SHAPEEE : ',y_true.shape)\n","print('SHAPEEE : ',y_pred.shape)\n","print(confusion_matrix(y_true,y_pred))\n","print('\\n\\nClassification Report : \\n\\n',classification_report(y_true,y_pred))\n","\n","y_true = np.argmax(y_testScream,axis=1)\n","y_pred = np.argmax(Fusedmodel.predict(x_testScream)[1],axis=1)\n","print('\\nConfusion Matrix SCREAM:\\n\\n')\n","print('SHAPEEE : ',y_true.shape)\n","print('SHAPEEE : ',y_pred.shape)\n","print(confusion_matrix(y_true,y_pred))\n","print('\\n\\nClassification Report : \\n\\n',classification_report(y_true,y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Ui0HMEfn_Dg","executionInfo":{"status":"ok","timestamp":1696395821969,"user_tz":300,"elapsed":9249,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}},"outputId":"036b77b2-cd97-4af6-a80b-84d6dda0d053"},"execution_count":159,"outputs":[{"output_type":"stream","name":"stdout","text":["Layer Name: conv2d_52\n","Trainable Parameters Count: 416\n","Layer Name: conv2d_53\n","Trainable Parameters Count: 12832\n","Layer Name: conv2d_54\n","Trainable Parameters Count: 51264\n","Layer Name: conv2d_55\n","Trainable Parameters Count: 204928\n","Layer Name: output1\n","Trainable Parameters Count: 258\n","Layer Name: output2\n","Trainable Parameters Count: 258\n","------------------------\n","Layer Name: conv2d_12\n","Trainable Parameters Count: 416\n","Layer Name: conv2d_13\n","Trainable Parameters Count: 12832\n","Layer Name: conv2d_14\n","Trainable Parameters Count: 51264\n","Layer Name: conv2d_15\n","Trainable Parameters Count: 204928\n","Layer Name: dense_3\n","Trainable Parameters Count: 258\n","------------------------\n","Transferred weights from layer 'dense_3' to layer 'output1'.\n","Transferred weights from layer 'dense_3' to layer 'output2'.\n","OUT Gunshot:  [2.6380538940429688, 0.14506328105926514, 2.492990255355835, 0.9448160529136658, 0.5050167441368103]\n","Training Accuracy for Gunshot:  0.9448160529136658\n","OUT Gunshot:  [2.4106714725494385, 0.10860927402973175, 2.3020622730255127, 0.9599999785423279, 0.5266666412353516]\n","Testing Accuracy for Gunshot:  0.9599999785423279\n","OUT Scream:  [4.391014575958252, 4.282922267913818, 0.10809140652418137, 0.4912000000476837, 0.9552000164985657]\n","Training Accuracy for Scream:  0.9552000164985657\n","OUT Scream:  [4.382222652435303, 4.114679336547852, 0.2675437927246094, 0.5286624431610107, 0.9299362897872925]\n","Testing Accuracy for Scream:  0.9299362897872925\n","5/5 [==============================] - 0s 36ms/step\n","\n","Confusion Matrix GUNSHOT:\n","\n","\n","SHAPEEE :  (150,)\n","SHAPEEE :  (150,)\n","[[74  6]\n"," [ 0 70]]\n","\n","\n","Classification Report : \n","\n","               precision    recall  f1-score   support\n","\n","           0       1.00      0.93      0.96        80\n","           1       0.92      1.00      0.96        70\n","\n","    accuracy                           0.96       150\n","   macro avg       0.96      0.96      0.96       150\n","weighted avg       0.96      0.96      0.96       150\n","\n","5/5 [==============================] - 0s 39ms/step\n","\n","Confusion Matrix SCREAM:\n","\n","\n","SHAPEEE :  (157,)\n","SHAPEEE :  (157,)\n","[[75  9]\n"," [ 2 71]]\n","\n","\n","Classification Report : \n","\n","               precision    recall  f1-score   support\n","\n","           0       0.97      0.89      0.93        84\n","           1       0.89      0.97      0.93        73\n","\n","    accuracy                           0.93       157\n","   macro avg       0.93      0.93      0.93       157\n","weighted avg       0.93      0.93      0.93       157\n","\n"]}]},{"cell_type":"markdown","source":["#GUN VS SIREN ARQ 4 MAX/MIN/AVG VALUE IN LAYER"],"metadata":{"id":"Eh7hSTAAneXU"}},{"cell_type":"code","source":["#1\t3\t4096\t4096\t4096\t0\t2\n","#2\t12\t4096\t4096\t4096\t0\t3\n","#3\t27\t4096\t4096\t4096\t0\t3\n","#4\t45\t4096\t4096\t4096\t0\t5\n","\n","samplerate = 22050\n","longitudMaxAudio = 4\n","Nmfcc = 45\n","Nfft = 4096\n","NwinL = 4096\n","iterableNhopL = 1.0\n","NhopL =  4096       #int(iterableNhopL*NwinL)\n","k_size = 5\n","num_rows = Nmfcc\n","num_columns = int(samplerate*longitudMaxAudio/NhopL) + int(samplerate*longitudMaxAudio/NhopL*0.05)  #Calculo longitud de salida de mfcc con 5% de tolerancia para longitud de audios\n","num_channels = 1\n","\n","num_labels = 2 #4\n","\n","#model = getModel(num_rows, num_columns, num_channels, num_labels, k_size)\n","#model = getModelFunctional(num_rows, num_columns, num_channels, num_labels, k_size)\n","Fusedmodel = getModelFunctionalModified(num_rows, num_columns, num_channels, num_labels, k_size)\n","\n","# Compila el modelo (es necesario antes de la evaluación)\n","Fusedmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","#TRAINED MODELS TO FUSE\n","modelGunshotTLSave_path_4 = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/Models/Fusion/TL_Complete/gunshot/saved_gunshot_TL_4'\n","modelGunshotTL_4 = tf.keras.models.load_model(modelGunshotTLSave_path_4)\n","\n","modelSirenTLSave_path_4 = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/Models/Fusion/TL_Complete/siren/saved_siren_TL_4'\n","modelSirenTL_4 = tf.keras.models.load_model(modelSirenTLSave_path_4)\n","\n","#layer_index = 3  # Specify the index of the layer (e.g., 0 for the first convolutional layer)\n","#change_filters(modelGunshotTL, modelScreamTL, 0, \"model1_to_model2\")\n","#change_filters(modelGunshotTL, modelScreamTL, 1, \"model1_to_model2\")\n","#change_filters(modelGunshotTL, modelScreamTL, 2, \"model2_to_model1\")\n","#change_filters(modelGunshotTL, modelScreamTL, layer_index, \"model1_to_model2\")\n","\n","# Iterar sobre las capas y verificar la cantidad de parámetros entrenables\n","for layer in Fusedmodel.layers:\n","    trainable_params_count = layer.count_params()\n","    if trainable_params_count > 0:\n","        print(\"Layer Name:\", layer.name)\n","        print(\"Trainable Parameters Count:\", trainable_params_count)\n","print(\"------------------------\")\n","\n","# Iterar sobre las capas y verificar la cantidad de parámetros entrenables\n","for layer in modelGunshotTL_4.layers:\n","    trainable_params_count = layer.count_params()\n","    if trainable_params_count > 0:\n","        print(\"Layer Name:\", layer.name)\n","        print(\"Trainable Parameters Count:\", trainable_params_count)\n","print(\"------------------------\")\n","\n","\n","#transfer_parameters_between_models(modelGunshotTL, 0, Fusedmodel, 0)\n","#transfer_parameters_between_models(modelGunshotTL, 1, Fusedmodel, 1)\n","#transfer_parameters_between_models(modelScreamTL, 2, Fusedmodel, 2)\n","#transfer_parameters_between_models(modelGunshotTL, 3, Fusedmodel, 3)\n","\n","\n","\n","#transfer_max_abs_conv_weights(modelGunshotTL_4, modelSirenTL_4, Fusedmodel)\n","#transfer_min_abs_conv_weights(modelGunshotTL_4, modelSirenTL_4, Fusedmodel)\n","transfer_avg_conv_weights(modelGunshotTL_4, modelSirenTL_4, Fusedmodel)\n","\n","transfer_parameters_between_models(modelGunshotTL_4, 4, Fusedmodel, 4)\n","transfer_parameters_between_models(modelSirenTL_4, 4, Fusedmodel, 5)\n","\n","\n","#Fusedmodel.summary()\n","\n","\n","# Evaluating the FUSED model on the training and testing sets\n","\n","scoreGunshot = Fusedmodel.evaluate(x_trainGunshot, [y_trainGunshot, np.resize(y_trainSiren, (y_trainGunshot.shape[0], *y_trainSiren.shape[1:]))], verbose=0)\n","print(\"OUT Gunshot: \", scoreGunshot)\n","print(\"Training Accuracy for Gunshot: \", scoreGunshot[3])\n","\n","scoreGunshot = Fusedmodel.evaluate(x_testGunshot, [y_testGunshot, np.resize(y_testSiren, (y_testGunshot.shape[0], *y_testSiren.shape[1:]))], verbose=0)\n","print(\"OUT Gunshot: \", scoreGunshot)\n","print(\"Testing Accuracy for Gunshot: \", scoreGunshot[3])\n","\n","\n","\n","scoreSiren= Fusedmodel.evaluate(x_trainSiren, [np.resize(y_trainGunshot, (y_trainSiren.shape[0], *y_trainGunshot.shape[1:])), y_trainSiren], verbose=0)\n","print(\"OUT Siren: \", scoreSiren)\n","print(\"Training Accuracy for Siren: \", scoreSiren[4])\n","\n","scoreSiren= Fusedmodel.evaluate(x_testSiren, [np.resize(y_testGunshot, (y_testSiren.shape[0], *y_testGunshot.shape[1:])), y_testSiren], verbose=0)\n","print(\"OUT Siren: \", scoreSiren)\n","print(\"Testing Accuracy for Siren: \", scoreSiren[4])\n","\n","\n","y_true = np.argmax(y_testGunshot,axis=1)\n","y_pred = np.argmax(Fusedmodel.predict(x_testGunshot)[0],axis=1)\n","print('\\nConfusion Matrix GUNSHOT:\\n\\n')\n","print('SHAPEEE : ',y_true.shape)\n","print('SHAPEEE : ',y_pred.shape)\n","print(confusion_matrix(y_true,y_pred))\n","print('\\n\\nClassification Report : \\n\\n',classification_report(y_true,y_pred))\n","\n","y_true = np.argmax(y_testSiren,axis=1)\n","y_pred = np.argmax(Fusedmodel.predict(x_testSiren)[1],axis=1)\n","print('\\nConfusion Matrix SIREN:\\n\\n')\n","print('SHAPEEE : ',y_true.shape)\n","print('SHAPEEE : ',y_pred.shape)\n","print(confusion_matrix(y_true,y_pred))\n","print('\\n\\nClassification Report : \\n\\n',classification_report(y_true,y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Oja3aXPBneGD","executionInfo":{"status":"ok","timestamp":1696396090222,"user_tz":300,"elapsed":14383,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}},"outputId":"4514fffc-d00c-490c-a89f-33ec7a5e79fd"},"execution_count":163,"outputs":[{"output_type":"stream","name":"stdout","text":["Layer Name: conv2d_64\n","Trainable Parameters Count: 416\n","Layer Name: conv2d_65\n","Trainable Parameters Count: 12832\n","Layer Name: conv2d_66\n","Trainable Parameters Count: 51264\n","Layer Name: conv2d_67\n","Trainable Parameters Count: 204928\n","Layer Name: output1\n","Trainable Parameters Count: 258\n","Layer Name: output2\n","Trainable Parameters Count: 258\n","------------------------\n","Layer Name: conv2d_12\n","Trainable Parameters Count: 416\n","Layer Name: conv2d_13\n","Trainable Parameters Count: 12832\n","Layer Name: conv2d_14\n","Trainable Parameters Count: 51264\n","Layer Name: conv2d_15\n","Trainable Parameters Count: 204928\n","Layer Name: dense_3\n","Trainable Parameters Count: 258\n","------------------------\n","Transferred weights from layer 'dense_3' to layer 'output1'.\n","Transferred weights from layer 'dense_3' to layer 'output2'.\n","OUT Gunshot:  [2.002363681793213, 0.13800734281539917, 1.8643566370010376, 0.9448160529136658, 0.49665552377700806]\n","Training Accuracy for Gunshot:  0.9448160529136658\n","OUT Gunshot:  [1.6107232570648193, 0.1036984995007515, 1.507024884223938, 0.9666666388511658, 0.5533333420753479]\n","Testing Accuracy for Gunshot:  0.9666666388511658\n","OUT Siren:  [5.818717002868652, 5.627828598022461, 0.19088983535766602, 0.5026918053627014, 0.9320322871208191]\n","Training Accuracy for Siren:  0.9320322871208191\n","OUT Siren:  [4.663945198059082, 4.452065944671631, 0.2118796408176422, 0.5349462628364563, 0.9247311949729919]\n","Testing Accuracy for Siren:  0.9247311949729919\n","5/5 [==============================] - 0s 22ms/step\n","\n","Confusion Matrix GUNSHOT:\n","\n","\n","SHAPEEE :  (150,)\n","SHAPEEE :  (150,)\n","[[75  5]\n"," [ 0 70]]\n","\n","\n","Classification Report : \n","\n","               precision    recall  f1-score   support\n","\n","           0       1.00      0.94      0.97        80\n","           1       0.93      1.00      0.97        70\n","\n","    accuracy                           0.97       150\n","   macro avg       0.97      0.97      0.97       150\n","weighted avg       0.97      0.97      0.97       150\n","\n","12/12 [==============================] - 0s 24ms/step\n","\n","Confusion Matrix SIREN:\n","\n","\n","SHAPEEE :  (372,)\n","SHAPEEE :  (372,)\n","[[168  24]\n"," [  4 176]]\n","\n","\n","Classification Report : \n","\n","               precision    recall  f1-score   support\n","\n","           0       0.98      0.88      0.92       192\n","           1       0.88      0.98      0.93       180\n","\n","    accuracy                           0.92       372\n","   macro avg       0.93      0.93      0.92       372\n","weighted avg       0.93      0.92      0.92       372\n","\n"]}]},{"cell_type":"markdown","source":["#SCREAM VS SIREN ARQ 4 MAX/MIN/AVG VALUE IN LAYER"],"metadata":{"id":"O5-cUowAnFK_"}},{"cell_type":"code","source":["#1\t3\t4096\t4096\t4096\t0\t2\n","#2\t12\t4096\t4096\t4096\t0\t3\n","#3\t27\t4096\t4096\t4096\t0\t3\n","#4\t45\t4096\t4096\t4096\t0\t5\n","\n","samplerate = 22050\n","longitudMaxAudio = 4\n","Nmfcc = 45\n","Nfft = 4096\n","NwinL = 4096\n","iterableNhopL = 1.0\n","NhopL =  4096       #int(iterableNhopL*NwinL)\n","k_size = 5\n","num_rows = Nmfcc\n","num_columns = int(samplerate*longitudMaxAudio/NhopL) + int(samplerate*longitudMaxAudio/NhopL*0.05)  #Calculo longitud de salida de mfcc con 5% de tolerancia para longitud de audios\n","num_channels = 1\n","\n","num_labels = 2 #4\n","\n","#model = getModel(num_rows, num_columns, num_channels, num_labels, k_size)\n","#model = getModelFunctional(num_rows, num_columns, num_channels, num_labels, k_size)\n","Fusedmodel = getModelFunctionalModified(num_rows, num_columns, num_channels, num_labels, k_size)\n","\n","# Compila el modelo (es necesario antes de la evaluación)\n","Fusedmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","#TRAINED MODELS TO FUSE\n","modelScreamTLSave_path_4 = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/Models/Fusion/TL_Complete/scream/saved_scream_TL_4'\n","modelScreamTL_4 = tf.keras.models.load_model(modelScreamTLSave_path_4)\n","\n","modelSirenTLSave_path_4 = '/content/drive/MyDrive/Colab Notebooks/Urban Sound Detection/Models/Fusion/TL_Complete/siren/saved_siren_TL_4'\n","modelSirenTL_4 = tf.keras.models.load_model(modelSirenTLSave_path_4)\n","\n","#layer_index = 3  # Specify the index of the layer (e.g., 0 for the first convolutional layer)\n","#change_filters(modelGunshotTL, modelScreamTL, 0, \"model1_to_model2\")\n","#change_filters(modelGunshotTL, modelScreamTL, 1, \"model1_to_model2\")\n","#change_filters(modelGunshotTL, modelScreamTL, 2, \"model2_to_model1\")\n","#change_filters(modelGunshotTL, modelScreamTL, layer_index, \"model1_to_model2\")\n","\n","# Iterar sobre las capas y verificar la cantidad de parámetros entrenables\n","for layer in Fusedmodel.layers:\n","    trainable_params_count = layer.count_params()\n","    if trainable_params_count > 0:\n","        print(\"Layer Name:\", layer.name)\n","        print(\"Trainable Parameters Count:\", trainable_params_count)\n","print(\"------------------------\")\n","\n","# Iterar sobre las capas y verificar la cantidad de parámetros entrenables\n","for layer in modelScreamTL_4.layers:\n","    trainable_params_count = layer.count_params()\n","    if trainable_params_count > 0:\n","        print(\"Layer Name:\", layer.name)\n","        print(\"Trainable Parameters Count:\", trainable_params_count)\n","print(\"------------------------\")\n","\n","\n","#transfer_parameters_between_models(modelGunshotTL, 0, Fusedmodel, 0)\n","#transfer_parameters_between_models(modelGunshotTL, 1, Fusedmodel, 1)\n","#transfer_parameters_between_models(modelScreamTL, 2, Fusedmodel, 2)\n","#transfer_parameters_between_models(modelGunshotTL, 3, Fusedmodel, 3)\n","\n","\n","\n","#transfer_max_abs_conv_weights(modelScreamTL_4, modelSirenTL_4, Fusedmodel)\n","#transfer_min_abs_conv_weights(modelScreamTL_4, modelSirenTL_4, Fusedmodel)\n","transfer_avg_conv_weights(modelScreamTL_4, modelSirenTL_4, Fusedmodel)\n","\n","transfer_parameters_between_models(modelScreamTL_4, 4, Fusedmodel, 4)\n","transfer_parameters_between_models(modelSirenTL_4, 4, Fusedmodel, 5)\n","\n","\n","#Fusedmodel.summary()\n","\n","\n","# Evaluating the FUSED model on the training and testing sets\n","\n","scoreScream = Fusedmodel.evaluate(x_trainScream, [y_trainScream, np.resize(y_trainSiren, (y_trainScream.shape[0], *y_trainSiren.shape[1:]))], verbose=0)\n","print(\"OUT Scream: \", scoreScream)\n","print(\"Training Accuracy for Scream: \", scoreScream[3])\n","\n","scoreScream = Fusedmodel.evaluate(x_testScream, [y_testScream, np.resize(y_testSiren, (y_testScream.shape[0], *y_testSiren.shape[1:]))], verbose=0)\n","print(\"OUT Scream: \", scoreScream)\n","print(\"Testing Accuracy for Scream: \", scoreScream[3])\n","\n","\n","\n","scoreSiren= Fusedmodel.evaluate(x_trainSiren, [np.resize(y_trainScream, (y_trainSiren.shape[0], *y_trainScream.shape[1:])), y_trainSiren], verbose=0)\n","print(\"OUT Siren: \", scoreSiren)\n","print(\"Training Accuracy for Siren: \", scoreSiren[4])\n","\n","scoreSiren= Fusedmodel.evaluate(x_testSiren, [np.resize(y_testScream, (y_testSiren.shape[0], *y_testScream.shape[1:])), y_testSiren], verbose=0)\n","print(\"OUT Siren: \", scoreSiren)\n","print(\"Testing Accuracy for Siren: \", scoreSiren[4])\n","\n","\n","y_true = np.argmax(y_testScream,axis=1)\n","y_pred = np.argmax(Fusedmodel.predict(x_testScream)[0],axis=1)\n","print('\\nConfusion Matrix SCREAM:\\n\\n')\n","print('SHAPEEE : ',y_true.shape)\n","print('SHAPEEE : ',y_pred.shape)\n","print(confusion_matrix(y_true,y_pred))\n","print('\\n\\nClassification Report : \\n\\n',classification_report(y_true,y_pred))\n","\n","y_true = np.argmax(y_testSiren,axis=1)\n","y_pred = np.argmax(Fusedmodel.predict(x_testSiren)[1],axis=1)\n","print('\\nConfusion Matrix SIREN:\\n\\n')\n","print('SHAPEEE : ',y_true.shape)\n","print('SHAPEEE : ',y_pred.shape)\n","print(confusion_matrix(y_true,y_pred))\n","print('\\n\\nClassification Report : \\n\\n',classification_report(y_true,y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gaNB708nnGWi","executionInfo":{"status":"ok","timestamp":1696396317774,"user_tz":300,"elapsed":15156,"user":{"displayName":"RICARDO TANGARIFE GONZALEZ","userId":"06954198022333239983"}},"outputId":"9a5816b6-ba5b-41be-e8f0-0bbf234a5753"},"execution_count":167,"outputs":[{"output_type":"stream","name":"stdout","text":["Layer Name: conv2d_76\n","Trainable Parameters Count: 416\n","Layer Name: conv2d_77\n","Trainable Parameters Count: 12832\n","Layer Name: conv2d_78\n","Trainable Parameters Count: 51264\n","Layer Name: conv2d_79\n","Trainable Parameters Count: 204928\n","Layer Name: output1\n","Trainable Parameters Count: 258\n","Layer Name: output2\n","Trainable Parameters Count: 258\n","------------------------\n","Layer Name: conv2d_12\n","Trainable Parameters Count: 416\n","Layer Name: conv2d_13\n","Trainable Parameters Count: 12832\n","Layer Name: conv2d_14\n","Trainable Parameters Count: 51264\n","Layer Name: conv2d_15\n","Trainable Parameters Count: 204928\n","Layer Name: dense_3\n","Trainable Parameters Count: 258\n","------------------------\n","Transferred weights from layer 'dense_3' to layer 'output1'.\n","Transferred weights from layer 'dense_3' to layer 'output2'.\n","OUT Scream:  [2.7214295864105225, 0.031180502846837044, 2.690248727798462, 0.9968000054359436, 0.5055999755859375]\n","Training Accuracy for Scream:  0.9968000054359436\n","OUT Scream:  [1.99534010887146, 0.0664723739027977, 1.9288678169250488, 0.9808917045593262, 0.5796178579330444]\n","Testing Accuracy for Scream:  0.9808917045593262\n","OUT Siren:  [2.904815673828125, 2.8188066482543945, 0.0860091969370842, 0.491251677274704, 0.9724091291427612]\n","Training Accuracy for Siren:  0.9724091291427612\n","OUT Siren:  [2.691321611404419, 2.5494024753570557, 0.14191879332065582, 0.5376344323158264, 0.948924720287323]\n","Testing Accuracy for Siren:  0.948924720287323\n","5/5 [==============================] - 1s 51ms/step\n","\n","Confusion Matrix SCREAM:\n","\n","\n","SHAPEEE :  (157,)\n","SHAPEEE :  (157,)\n","[[82  2]\n"," [ 1 72]]\n","\n","\n","Classification Report : \n","\n","               precision    recall  f1-score   support\n","\n","           0       0.99      0.98      0.98        84\n","           1       0.97      0.99      0.98        73\n","\n","    accuracy                           0.98       157\n","   macro avg       0.98      0.98      0.98       157\n","weighted avg       0.98      0.98      0.98       157\n","\n","12/12 [==============================] - 1s 43ms/step\n","\n","Confusion Matrix SIREN:\n","\n","\n","SHAPEEE :  (372,)\n","SHAPEEE :  (372,)\n","[[181  11]\n"," [  8 172]]\n","\n","\n","Classification Report : \n","\n","               precision    recall  f1-score   support\n","\n","           0       0.96      0.94      0.95       192\n","           1       0.94      0.96      0.95       180\n","\n","    accuracy                           0.95       372\n","   macro avg       0.95      0.95      0.95       372\n","weighted avg       0.95      0.95      0.95       372\n","\n"]}]}]}